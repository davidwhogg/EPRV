% This file is part of the EPRV project.
% Copyright 2017, 2018, 2019 the authors.

% Style notes
% -----------
% - Re-define acronyms in each section? So less flippy flippy? Yes that's the thing.?
% - 

% To-do items
% -----------
% - search for all \todo
% - search for all HOGG
% - search for all BEDELL
% - search for all CITE
% - should we fun-cite some ancient papers in the first paragraph, like Hubble and Oort?

\documentclass[modern]{aastex62}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[sort&compress]{natbib}
\usepackage[hang,flushmargin]{footmisc}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\m}{\unit{m}}
\newcommand{\cm}{\unit{cm}}
\newcommand{\s}{\unit{s}}
\newcommand{\kms}{\km\,\s^{-1}}
\newcommand{\mps}{\m\,\s^{-1}}
\newcommand{\cmps}{\cm\,\s^{-1}}
\newcommand{\ang}{\text{\normalfont\AA}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}

% text macros
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\renewcommand{\paragraph}[1]{\bigskip\par\noindent\textsl{#1} ---}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work
\newcommand{\CITE}{\todo{CITE}}
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\HARPS}{\project{\acronym{HARPS}}}
\newcommand{\HIRES}{\project{\acronym{HIRES}}}
\newcommand{\RV}{\acronym{RV}}
\newcommand{\CRLB}{\acronym{CRLB}}
\newcommand{\foreign}[1]{\textsl{#1}}

\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-0.4in}
\setlength{\parindent}{1.4em} % trust in Hogg
\shorttitle{data analysis for radial-velocity measurement}
\shortauthors{bedell and hogg}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
\graphicspath{ {figures/} }
\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{\textbf{How to measure maximally precise radial velocities from a set of spectra:\\
               The time-independent spectrum case}}

\author[0000-0001-9907-7742]{Megan Bedell}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Department of Astronomy \& Astrophysics, University of Chicago, 5640 S. Ellis Ave, Chicago, IL 60637, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Extremely precise stellar radial velocity (\RV) measurements
have been used to discover hundreds of exoplanets, and
many new spectrographs are being built for exoplanet search and
characterization.
The best published results show empirical precision at the $\mps$ level.
Information theory suggests that it ought to be possible ot reach
higher precisions with current observations.
% aims
Here we ask: What data-analysis methods deliver \RV\ measurements that
saturate the Cram\'er--Rao bound (\CRLB) on precision, given an
extracted spectrum (and associated uncertainties) from a spectrograph?
We presume that the stellar spectrum does not vary with time (this is
incorrect in detail).
% methods
We consider a smoothed (or
Gaussian-fit) cross-correlation with a binary mask,
cross-correlation with a stellar template,
and maximum-likelihood methods.
We draw out the relationships between these.
We consider both theoretically generated and data-driven templates.
% results
We find (as expected) that cross-correlation with a correct stellar
template produces \RV\ measurements that saturate the \CRLB.
We also find that 
the standard practice of fitting a Gaussian to the cross-correlation
with a binary mask saturates the \CRLB, so long as the binary mask is
correct in a certain sense.
As the template or mask becomes incorrect, \RV\ precision suffers.
Un-corrected telluric absorption lines can also harm \RV\ precision.
Most importantly, we find that using the data themselves to generate
the stellar and telluric templates does not cost any significant
information, once the number of observational epochs is large.
That is, data-driven stellar and telluric templates can be used to
saturate the \CRLB, provided that
the star does not vary substantially between epochs.
If the star does vary substantially, a lot more needs to be known
before the relevant \CRLB\ even can be computed.
\end{abstract}

\keywords{\raggedright
atmospheric~effects
---
binaries:~spectroscopic
---
methods:~data analysis
---
methods:~statistical
---
techniques:~radial~velocities
---
planets~and~satellites:~detection
}

\section*{}
\clearpage
\section{Introduction}

It is ridiculous to open this \documentname\ by pointing out that
the spectroscopic measurement of radial velocity (\RV) has been a
critical tool across many subfields of astronomy, from Galactic
dynamics to large-scale structure and cosmic expansion.
Fundamentally, the accuracy of radial-velocity measurement is limited
jointly by the signal-to-noise of the spectroscopic data and
the accuracy (in spectral space) of the spectral models.
In recent decades, however, a new focus has emerged on \emph{extremely
  precise} but \emph{purely relative} \RV\ measurements of stars as a
means of detecting exoplanets and other kinds of companions.
That is, we are looking for \emph{precision, not accuracy}.
For this application, measuring the absolute velocity of the star is
much less important than resolving changes through time.
This is the problem we address in this \documentname.

Earth-like planets induce $0.1\,\mps$-level signals in their host
stars!
Substantial hardware, software, and systems-engineering effort has
been put into the challenges of making measurements at this level of
precision (\CITE some reviews).
It appears that contemporary spectrographs are stable at the
$0.05\,\mps$ level (\CITE THINGS), and that the information content in
the data---which is set by the spectral features, the spectrograph
resolution, the wavelength range, and the observational
signal-to-noise (\CITE THINGS)---is sufficient.
And yet, the best published \RV\ data sets show empirical scatter
(rms variations around best-fit kinematic models, for example) at the
$1\,\mps$ level (for example, \CITE THINGS).
We hear rumors of various projects delivering empirical scatters for
the best stars at the $0.2$ to $0.5\,\mps$ level
(J.~Brewer \& D.~Fischer, private communication; F.~Pepe, private communication)
but they aren't yet published.
One of the motivating questions for this \documentname\ is: Is it possible that
the data-analysis methodologies currently in use are contributing to this
empirical variance?
That is, are we getting the precision we should be, given the data
quality?
This is a question in information theory and inference.

While the general idea of measuring a Doppler shift by the relative
motions of stellar absorption lines is fairly straightforward, there
are considerable complexities and subtleties.
These start at the design of the spectrograph hardware itself
and then go on to include 
the observing strategy, calibration methodology, spectral extraction,
choice of reference spectrum or template, and treatment of telluric absorption
features.
Here we will focus on the last parts of this: We will presume that the
spectrograph is delivering wavelength-calibrated, extracted, continuum-normalized
spectra with minimal issues.
We will consider how to obtain the most
precise possible \RV\ measurements from those good spectra.
A more complete treatment would consider the hardware, observing strategy,
calibration methodology, spectral extraction, and \RV\ measurement as
an integrated system of hardware, software, and operations.
That's an important direction for future research, but out of scope for this \documentname.

Ultra-stabilized spectrographs like \HARPS\ \todo{(define \& \CITE)}
predominantly utilize a cross-correlation method with a custom-built
list of spectral lines (see e.g. Queloz1995, Baranne1996).
A lesser-used alternative is cross-correlation or maximum-likelihood
fitting using a template spectrum generated from the data \todo{(Buchhave,
Anglada-Escude)}.
This often involves masking or downweighting of the most
telluric-contaminated spectral regions.
As an alternative, gas-cell-calibrated spectrographs like
\HIRES\ \todo{(CITE)} commonly adopt more of a forward-modeling approach.
In this framework, the instrumental line spread function, the stellar
RV, and other parameters are optimized by modeling each observation as
a combination of shifted and instrument-convolved template spectra for
the star and gas cell (\CITE).
Here we focus on the ultra-stabilized spectrograph hardware, but consider
a few different data-analysis approaches.
Most of what we do here could be generalized to data generated by
gas-cell hardware; we will return to this point in the discussion.

For the purposes of this \documentname, we will assume that the data
set at hand consists of multiple observations of the same star with
good coverage throughout the observing season.
We will assume that the resolution of the spectrograph is $(\lambda /
\Delta\lambda) > 30,000$, and the star is not near the ecliptic pole,
so that the observing coverage also delivers significant RV shifts
between star and atmosphere (and hence between star and tellurics).
We are not onsidering the case of a real-time data reduction pipeline,
because such a real-time pipeline does not, at the beginning of an
observing campaign, have many epochs at hand.

We begin in \sectionname~\ref{s:info} with a derivation of the information-theory-based limit on the RV information content of a spectrum. 
In \sectionname~\ref{s:methods}, we summarize common RV extraction practices used by current \RV\ pipelines, including cross-correlation and template fitting. 
We then implement these techniques to extract \RV s using 5-\ang-wide sections of simulated and real spectra.
The simulated and real data are described in \sectionname~\ref{s:data}. 
We compare the relative performances of the methods in \sectionname~\ref{s:experiments} and conclude with some recommendations for future RV pipelines in \sectionname~\ref{s:summary}. 

\section{Information Theory}
\label{s:info}

Given finite, noisy data, there is a limit coming from information
theory on how well any parameter of interest can be measured.
This limit is
sometimes known as the Cram\'er--Rao Lower Bound (\CRLB; \todo{cite CR and wikipedia}).
In its simplest form, it constrains the variance of any unbiased
frequentist estimator.
Estimators can only beat this bound by taking on bias.

This bound is strong, but it cannot be calculated \emph{objectively};
any description of the information in a data set requires
subjective decisions (that is, debatable decisions), which are inputs
to a model for the generation of the data.
In particular, information-theory bounds require a model for the
probability of the data given the parameter of interest; that is, they
require a likelihood function.
As an aside, it is worth noting that
Bayesians need a likelihood function for inference, and
frequentists need a likelihood function for analyzing their estimators.
The likelihood function is therefore central to all statistical methods,
and the core component of any data analysis.

For our purposes, the \CRLB\ is that the \emph{inverse variance} $\sigma_v^{-2}$ of any velocity
estimator (that is, the information in the velocity estimator) must not be larger than the
second derivative of the \emph{negative} log likelihood function:
\begin{equation}
\frac{1}{\sigma_v^2} \leq E\left[\frac{\dd^2}{\dd v^2}(-\ln L)\right] \quad,
\end{equation}
where $L$ is the likelihood, or the probability of the data given the radial velocity $v$,
and the expectation $E[\cdot]$ is taken over all possible data (that is, it is an
expectation under the likelihood).
This expectation value of a second derivative is the Fisher Information
(information has units of inverse variance).
This is the information-theory object that is frequently used in
cosmological experiment forecasts (\todo{CITE SHIT}),
and it is the quantity used for what we often call the ``formal error'' or
``formal uncertainty'' on a least-square fit (\todo{CITE fitting a line}).
The bound is an upper bound on the inverse variance, and thus a lower bound
on the uncertainty.

If there are many nuisance parameters, a Bayesian can marginalize them
out to make $L$ the marginalized likelihood for the velocity, or a frequentist
can include them in the estimation and compute an inverse covariance matrix $C^{-1}$
for the full parameter vector.
This will then will have the bound
\begin{equation}
u^T\cdot C^{-1}\cdot u \leq u^T\cdot Q\cdot u
\end{equation}
\begin{equation}
Q_{ij} \equiv E\left[\frac{\dd^2}{\dd\theta_i\,\dd\theta_j}(-\ln L)\right] \quad,
\end{equation}
where $Q$ is the information tensor, $i,j$ are indices into that
tensor, corresponding parameters are $\theta_i, \theta_j$, and $u$ is
any arbitrary vector.
That is, the bound is not just on every parameter, but really on on
any combination of parameters.

In the case of extreme-precision \RV, this says that any computation of the limits
on estimating stellar velocities (or stellar velocity changes)
requires a likelihood function.
This, in turn, requires making assumptions about the stellar spectrum (the
latent, unobserved, \emph{true} spectrum that could only be observed
with arbitrarily good data) and the noise processes that distort the
spectrum in any finite, noisy observation.
These noise processes ought (in principle) to include any beliefs about
unsubtracted telluric absorptions or emissions, and time variability
of the star itself, not to mention photon and spectrograph read and
extraction noise.
However---and in keeping with the literature---we can make maximal or
best-case assumptions, and ask what we would get for \RV\ precision
in that best case.
Then we can ask (with fake data) how much different choices about our
data analysis, and different adversarial injections of noise,
tellurics, spectrograph issues, and other wrongness can prevent us
from achieving the best-case bounds.

The most optimistic assumptions we can possibly make are the following.
\begin{itemize}
\item The spectrograph is perfectly calibrated in a wavelength sense.
\item The spectrograph is consistently and repeatably calibrated in a
  flux sense. That is, the spectrum does not need to be perfectly
  continuum-normalized or perfectly flux-normalized, but it is
  calibrated near-perfectly \emph{consistently} across epochs (exposures).
  We will break this assumption in our experiments, below.
\item There are no residual telluric absorptions nor any residual sky
  emission in the spectra. That is, the sky is near-perfectly calibrated
  and removed.
  We will also break this assumption in our experiments, below.
\item The \emph{true} spectrum (that is, the latent spectrum that
  would be observed if the data were far, far better) of the star is
  known at better accuracy and precision than the main noise sources.
  We will break this assumption too.
  The whole point of this paper is to ask whether we can create a
  good-enough proxy for the true spectrum.
\item There is no time dependence (no epoch-to-epoch variations) in the
  stellar spectrum.
\item All noise sources contributing to the spectral data---which
  include at a minimum photon noise, read noise, residual telluric
  issues, residual wavelength-calibration issues, and residual stellar
  spectral variability---are, when summed together, indistinguishable
  from being zero-mean, Gaussian, and additive.
\item The complete variance tensor of the total noise on the spectrum is known, and
  known correctly. That is, the variance of the noise at every pixel
  is known, as is the covariance between nearby pixels, and in the
  same \emph{true} sense that the true spectrum of the star is known.
\end{itemize}
It is worthy of note that every formula (that we know) in the
literature for maximum \RV\ precision (\todo{cite Butler, Bouchy,
  Figueira, Lovis}) makes all of these same assumptions,
plus additional \emph{even more restrictive assumptions}.
Under these disturbingly unrealistic assumptions, the \CRLB\ becomes
\begin{equation}\label{eq:crlb}
\frac{1}{\sigma_v^2} \leq \left[\frac{\dd f(v)}{\dd v}\right]\T\cdot C^{-1}\cdot\left[\frac{\dd f(v)}{\dd v}\right]
\end{equation}
where the derivatives are of the true spectrum $f(v)$ with respect to
radial velocity $v$ (which can, in turn, be thought of as the derivative of a Doppler
operator with respect to velocity acting on the true spectrum; \CITE WOBBLE), the
derivatives are column vectors, and $C^{-1}$ is the inverse of the covariance matrix
describing the true variance and covariances of the noise process.

Formulae (bounds) related to (\ref{eq:crlb}) have appeared previously
in the literature, although in somewhat more complicated forms.
Butler (\todo{cite}) shows a similar expression, but makes the additional
assumption that every pixel is independent; that is, that there is no
covariance between nearby pixels from, say, extraction or continuum normalization.
Bouchy (\todo{cite}) shows an expression that assumes that the pixels
are independent and that the noise is dominated by a combination of photon
and read noise. 
That work also replaces the model spectrum with a set of optimal
weights for cross-correlation, which amounts to very much the same thing,
as we will discuss below.
Figueira (\todo{cite}) and \todo{others (\CITE)} show a per-line
bound, based on assumptions similar to those of Bouchy but with the additional assumption that
each line is well approximated by a Gaussian (squared exponential) in shape.
These literature bounds are all extremely similar, and similar to (\ref{eq:crlb});
nothing here is controversial.
We will come back to the scaling of uncertainty with various
Gaussian line parameters in \sectionname~\ref{s:singleline}.

Now that we have established the bound, how can we saturate it? That is,
how can we make measurements (estimators) with variances that come close to the \CRLB?
The short answer is that reliable bound-saturating estimators are maximum-likelihood estimators.
The long answer is long, but part of it is that the bound can only be saturated if the
assumptions are correct, and note that the assumptions include that we know the
\emph{true} spectrum of the star, among many other things.
That is, if we can take data that come close to meeting the assumptions, and
if we can find a spectral model that comes close to the true spectrum of the star,
then in principle we can estimate radial velocities with a precision close to the \CRLB.

Given that computational stellar spectral models have physical issues, the best way
to get an accurate estimate of the true spectrum of the star is going to be data-driven.
In what follows, we are going to show that---for a typical \HARPS-like observing program---%
it is possible to build a data-driven model of the stellar spectrum that is good enough
to nearly-saturate the \CRLB.
That is, with a data-driven model of the true stellar spectrum, model inaccuracy will
not dominate the \RV\ noise budget.
We will additionally bring evidence that telluric features can be a significant problem,
but that they also can plausibly be mitigated with a data-driven model.

\section{Radial-velocity estimators}
\label{s:methods}

There are three dominant methods for the estimation of a radial veloctity,
given spectral data.

\paragraph{Maximum-likelihood estimator}
\CRLB-saturating estimators are (or are equivalent to)
maximum-likelihood estimators.
When the likelihood is Gaussian, the log-likelihood
is a sum of a quadratic chi-squared-like term and a log-determinant term
\begin{equation}
\ln L = -\frac{1}{2}\,[y - f(v)]\T\cdot C^{-1}\cdot [y - f(v)] - \frac{1}{2}\,\ln||2\pi\,C||
\quad ,
\label{eq:lf}
\end{equation}
where $y$ is a column vector of the data, $f(v)$ is the model spectrum,
which depends on the \RV\ $v$,
and $C$ is the covariance matrix describing the noise variance.
In the case of independent pixel noise the chi-squared term just becomes an
inverse-variance weighted sum of squared residuals (chi-squared).
The maximum-likelihood estimator for $v$ is the value of $v$ which maximizes
the log-likelihood.
Since only the chi-squared term depends on $v$, maximizing the likelihood
means minimizing chi-squared.
Optimization is straightforward; it is one-dimensional optimization
(given the template spectrum $f(v)$, and derivatives with respect to
velocity are easy to take:
\begin{equation}
\frac{\dd\ln L}{\dd v} = [\frac{\dd f(v)}{\dd v}]\T\cdot C^{-1}\cdot [y - f(v)]
\quad .
\label{eq:dldv}
\end{equation}

\paragraph{Cross correlation with a template}
Most methods for radial-velocity estimation aren't explicitly maximizing
any likelihood.
Instead, they involve some kind of
cross-correlation of the data with a template.
This might be surprising, since (as we noted above) bound-saturating
estimators will be maximum-likelihood estimators.
However, under simple assumptions, cross-correlation optima are
also maximum-likelihood optima. The argument is as follows:
If we expand the quadratic form (chi-squared term) in the
likelihood in equation~(\ref{eq:lf}), it becomes
\begin{equation}
 {}-\frac{1}{2}\,y\T\cdot C^{-1}\cdot y
            + f(v)\T\cdot C^{-1}\cdot y
-\frac{1}{2}\,f(v)\T\cdot C^{-1}\cdot f(v) \quad.
\end{equation}
That is, the chi-squared term factors into a term that is the square
of the data $y$, a term that is the square of the model $f(v)$,
and a term that is a scalar product or projection or
cross correlation of data with model.
As long as the amplitude of $f(v)$ doesn't depend strongly on velocity $v$,
and the $C$ tensor is ``smooth'' in certain respects, the dependence of this
chi-squared term on velocity will be dominated by the
cross-correlation.
For this reason, we can use a variant of cross-correlation
optimization as a radial-velocity estimator, or, in other words, the
\RV\ $v$ that optimizes $f(v)\T\cdot C^{-1}\cdot y$ will also (nearly)
optimize the likelihood.

Another argument is as follows:
As long as the derivative $\dd f(v)/\dd v$ is close to orthogonal
(given the $C^{-1}$ metric) to the template $f(v)$, the derivative of
the log likelihood given in equation~(\ref{eq:dldv}) is equal to the
derivative of the cross-correlation $f(v)\T\cdot C^{-1}\cdot y$.
Therefore, whatever optimizes the likelihood will (nearly) optimize
the cross-correlation.

In actually performing this cross-correlation optimization,
investigators make some non-trivial choices.
The first is whether or not to use the inverse variance tensor $C$.
Many codes do not make any use of the uncertainty estimates at all in
the calculation of the cross correlation.
Not using the uncertainties (treating $C$ as the identity) is
equivalent to assuming that the pixel uncertainties don't vary much
from pixel to pixel across the spectrum.
If the cross correlation is optimized with $C$ set to the identity,
the optimum of that doesn't strictly optimize the likelihood function;
the differences depdends on the variations in the uncertainties.
Another choice is how finely to sample in radial velocity $v$, or how
to perform the optimization.
If the $v$ spacing is coarse (but not too coarse!), quadratic
interpolation can be used to find the optimum at high precision.
The velocity spacing need only be smaller than something like half the
spectral velocity resolution ($c\,\Delta\lambda / \lambda$).
This problem is similar to stellar centroiding methods in use in
imaging surveys (\CITE VAKILI).
Another choice faced by an investigator is what to do at the ends or
edges of the spectra or spectral segments used for the
cross-correlation.
If the edges are treated naively, it is possible to introduce unintended
dependencies that invalidate the requirement that the model-times-model term
(the $f(v)\T\cdot C^{-1}\cdot f(v)$ term) not depend on \RV\ $v$.

\todo{HOGG ...In detail, in the experiments that follow, the
cross-correlation we will optimize has the following properties...}

\paragraph{Cross correlation with a binary mask}
The maximum-likelihood and cross-correlation methods only give
bound-saturating \RV\ estimates when the spectral model $f(v)$
respresents something similar to the true expecation of the spectrum
at velocity $v$.
And yet there is a paradox, because many world-class \RV\ pipelines
(like for instance \\CITE) perform cross-correlation with a
\emph{binary mask}.
This is not a good representation of the spectral expectation! So why
do these methods deliver high quality \RV\ estimates?
The answer is that the codes that cross-correlate with a binary mask
subsequently fit that cross-correlation with a Gaussian function.
It turns out that the operation ``cross-correlate the data with a
binary mask, followed by fitting a Gaussian to the cross-correlation
function'' is mathematically equivalent to ``smooth the binary mask
with a Gaussian, and then cross-correlate with the data''.
That is, when a pipeline cross-correlates with a binary mask and then
fits that cross-correlation function with a Gaussian, it is equivalent
to smoothing the binary mask with a best-fitting Gaussian function to
make a smooth, realistic $f(v)$ and then cross-correlating with that
realistic $f(v)$.
The fitting gives the flexibility to match the spectral resolution of
the template to the spectral resolution of the data, in the
approximation that the resolution is close to constant over the
spectral range in velocity units.
So in all cases of bound-saturating \RV\ measurements, the
\RV\ estimation is effectively a good approximation to
maximum-likelihood with a good spectral template.

\paragraph{Choice of template}
A variety of choices exist for the cross-correlation template
$f(v)$. A few possibilities include:
\begin{itemize}
\item \emph{A theoretically produced synthetic spectrum:}
  This will be a slightly wrong model prediction,
  since no stellar model will produce perfect reproduction of the
  spectral expectation for the star in question.
  The chosen template should be matched as closely as possible however
  to the target star. Some surveys cross correlate with one universal
  template for all stars; this will lead to increases in both bias and
  variance relative to the \CRLB.
\item \emph{A binary mask:}
  This is a pseudo-spectrum of zero values with non-zero top-hats at the
  expected rest-frame wavelengths of the spectral lines of interest.
  A cross-correlation between this mask and the observed spectrum will
  effectively compute the average line profile with its minimum
  corresponding to the best-fit RV.
  As we note above, fitting that mean line profile with a Gaussian
  before identifying the \RV\ estimate is equivalent to smoothing the
  mask with a Gaussian and then using it as the stellar template in a
  straightforward cross correlation.
  The binary-mask method has the benefit of exactly zeroing out
  telluric-contaminated or otherwise unreliable regions of the
  spectrum (by deleting them from the mask).
  A desired relative weighting of lines can be incorporated by setting
  the non-zero mask values to different amplitudes at different lines
  (\CITE Pepe). This method is currently used by the \HARPS\ pipeline
  (\CITE), and an example is shown in \figurename~\ref{fig:binarymask}.
\item \emph{A data-driven estimate of the template spectrum:}
  Some surveys (for example, \HIRES, \CITE) use a single observation as a
  template.
  Combining information from many visits is better because the
  combined SNR will be higher.
  In what follows, we show that with sufficient epochs, a data-driven
  template is accurate enough to saturate the \CRLB.
\end{itemize}
\begin{figure}[htp]
\centering
% \includegraphics[width=\columnwidth]{binarymask}
\caption{Illustration of a weighted binary mask (blue windows) plotted over a small segment of the solar spectrum (black). The vertical length of each mask window corresponds to the weight given to the line when performing a cross-correlation between mask and spectrum.}
\label{fig:binarymask}
\end{figure}

\section{Data}
\label{s:data}

\todo{HOGG SYNTACTICAL}

The experiments we perform in this paper primarily use artificial data, for the simple reason that we require access to the underlying true RVs in order to assess the performance of our methods. 
We attempt to generate the fake data as realistically as possible for the case of a high-resolution, high-signal-to-noise spectrum of a Sun-like star taken with \HARPS. 
With these data, we are able to compute the \CRLB.  
We can also isolate the effects of various confounding noise sources like telluric features and imperfect continuum normalization by injecting them into the artificial data, which is generated free of correlated noise by default.

We will also show tests on real \HARPS\ data for two stars, one hosting a companion and
one (apparently) companion-free (at least at our precision). 
These tests serve as a demonstration that the methods presented here are useful when applied to real data and the assumptions made in our artificial data generation are realistic.

\subsection{Artificial Data}

The basic model that we use to generate fake spectra is extremely simple, assuming a perfectly normalized continuum; isolated, perfectly Gaussian spectral lines; and white noise only. 
We simulated a 5-\ang\ region with random noise corresponding to an SNR of 100, a typical value for a single \RV\ spectrum. 
Lines were inserted as Gaussians with prescribed centers, widths, and depths. 
Every line was convolved with the same line spread function, which was taken to be a Gaussian with $\sigma$ = 0.05 \ang. 
In the simplest test case, an arbitrary single line at 5000 \ang\ was used (see \sectionname~\ref{s:singleline}). 
However, for most of our tests we injected a set of 12 lines with centers between 4998 - 5002 \ang. 
The Gaussian parameters for these lines were based on fits to a reflected Solar spectrum from Vesta taken by \HARPS. \todo{(find citation/program number)} 
The resulting spectrum strongly resembles a 5-\ang\ chunk taken from a typical \HARPS\ spectrum of a Sun-like star (Figure \ref{fig:spectra}).

We repeated this synthesis to make a set of \todo{64} spectra, each with a random \RV\ between $-30$ and $30\,\kms$ (the approximate amplitude of the yearly \RV\ shifts induced by barycentric motion). 
The \RV s were applied as a multiplicative Doppler shift on the central wavelength of each spectral line to be injected. 
At this point, we are able to run a cross-correlation analysis on the set of spectra and examine the deviations in the recovered \RV s from the input \RV s.

\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{spectra}
\caption{}
\label{fig:spectra}
\end{figure}

\subsection{\HARPS\ Data}

munging notes

reference to figure comparing real \& fake data

\section{Experiments and results}
\label{s:experiments}

\subsection{Single-Line}
\label{s:singleline}

We began with the simplest test case possible: a single, strong absorption feature.

What is the CRLB?

Discuss scaling of the CRLB with line FWHM, constrast/depth, etc. According to Lovis \& Fischer (2010) and Figueira (2017), $\sigma_{RV}$ should go as $\frac{\sqrt{FWHM}}{SNR \times C}$, where $C$ is the line contrast, or the depth of the line as a fraction of the continuum level. Do we see this?

\subsection{Binary Mask}

Our next test involved a more realistic test spectrum. Using \HARPS\ spectra of the solar twin star HIP 54287, we measured the centers and depths of 12 lines in a 4-$\ang$ window around 5000 $\ang$. We used these measurements to generate a synthetic spectrum that closely resembles a typical observation of a Sun-like star.

What is the CRLB for this case? How close can we come to saturating it when using (a) a typical HARPS-like mask and (b) an optimized mask? In all cases, binary mask cross-correlation is very far from saturating the CRLB. Talk a little bit about why.

\subsection{Rigid Template}

how well does the mask/template approximate the real data? try fudging the relative line depths and locations believably, like maybe you have a slightly wrong spectral type or metallicity in the template. do a correct template and an incorrect template.

even a tiny wrongness to the template is visible in the RV variance.

\subsection{Data-Driven Template}

how do we construct the template? show that it comes close to saturating the CRLB.

comment: there are far better ways to derive the data-driven template. we leave this to future work.

\subsection{Realistic Sources of Structured Noise}

additional noise considerations:
\begin{itemize}
\item (micro-) telluric features: tellurics that you can't identify are important
\item subtle continuum trends: not that big a deal
\item cosmic rays?: we might skip this and add to discussion
\item additive signals?: for example sky emission, lunar and other reflected solar; we might skip this and just add to discussion
\end{itemize}

\subsection{Real Data}

all the data-driven code works on real data. we get a better RMS on the star from the data-driven template than from the binary mask. based on this, we believe that the \HARPS\ pipeline doesn't strictly use a binary mask.

\section{Summary and discussion}
\label{s:summary}

This \documentname\ reports good news for exoplanet-related
radial-velocity (\RV) experiments.
First and foremost, we have shown that a data-driven model for
stellar spectra can be accurate enough to deliver
\RV\ measurements that saturate information-theoretic
bounds.
It is not surprising that an accurate model of a stellar spectrum
yields good cross-correlation \RV\ measurements.
What is interesting (at least to us) is that there is sufficient
spectral information in a typical observing
program to determine the mean stellar spectrum to sufficient
quality that the \RV\ measurements are close to the best that can
theoretically be made.
It means that the information in the data that goes into determining
the mean stellar spectrum does not come at the expense (in any sense)
of the \RV\ information.
It also means that \RV\ experiments do not rely on having good models
of stellar spectra; no physical model of a star or its photosphere
is required.

...insert here one paragraph for each of the other major results from
the abstract. Each paragraph should emphasize the awesome of the
result, and/or discuss limitations or interpretation...

In \sectionname~\ref{s:info} we listed some exceedingly
optimistic assumptions under which we could compute the
information-theoretic limit on radial-velocity precision.
Some of these assumptions are worthy of discussion: How true are they,
and what happens, both from an information-theory perspective and from
a methodological perspective, when we relax them?

One of these assumptions, for example, is that the true stellar
spectrum is constant with time; that is, the variations from
epoch-to-epoch are from noise or nuisances (like telluric absorption).
This assumption is critical and deep.
The entire edifice of extreme-precision \RV\ is built on this assumption:
With imperfect models of stellar structure and photospheres, it is
impossible to know the \emph{absolute} \RV\ of a star to high precision;
we satisfy ourselves that we can measure \emph{relative} velocities
to arbitrarily high precision.
Relative precision---and everything in this paper---depends on our
ability to measure shifts from epoch to epoch.
If the stellar spectrum is permitted (or observed) to change from epoch
to epoch, there is no fundamental way to determine or confirm spectral
shifts corresponding to Doppler shifts.
That is, even relative \RV\ measurements depend on a constancy of the
stellar spectrum.
This is a deep issue, extending far beyond the scope of this paper.
Time-variable stellar spectra are not killing to the extreme-precision
\RV\ program!
But at current practices they may be limiting precision,
and to ameliorate their effects will require substantial and non-trivial adjustments to the
\foreign{status quo}.
We hope to address these in future work.

We also assumed that the noise is zero-mean, Gaussian, and with known
variance tensor.
There are certainly sources of noise that might violate these
assumptions.
There are two ways in which we might weaken our assumptions about
the noise.
The first is that the total noise acting on the system---and recall
that noise for us is anything not accounted for in our model of the
data---might not be Gaussian in form.
This will happen if the photon counts (per pixel) get low.
But it can also happen if there are unmodeled calibration artifacts
that don't happen to have Gaussian-distributed amplitudes.
This is not a problem in principle with computing or saturating the
information-theory limit (\CRLB).
The only significant change is that the simple Gaussian likelihood must
be replaced with the appropriate function.
That change modifies the \CRLB\ calculation, and it also modifies the
relevant data-analysis method, since cross-correlation (justified in
\sectionname~\ref{s:methods}) is only appropriate when the noise is
close to Gaussian.
In a non-Gaussian world, the \RV\ estimation must proceed by explicit
optimization of the likelihood (or Bayesian methods derived therefrom).

The second way in which we might weaken our assumptions about the
noise is that there might be contributions to the noise (especially
those coming from unmodeled calibration artifacts) with unknown
variance.
That is, it is an optimistic assumption that we know the variance of
the noise, and all the covariances.
Our only comment about this problem is that, at least, the assumption
is testable:
It is possible with big observing programs to look at the empirical
properties of the residuals away from the best-fit model.
Do the residuals appear to have zero mean and reasonable empirical
variance, given the assumed variance?
These tests are like chi-squared tests; they test the completeness
of the model.
If the model is incomplete, it can be fixed by adding new components
to the model (calibration vectors of various kinds, for example) or
inflating estimates of noise variance. Or both.

...Hogg: insert some grand final statement.

\acknowledgements
It is a pleasure to thank
  Jacob Bean (Chicago),
  John Brewer (Yale),
  Dan Foreman-Mackey (Flatiron),
  Ben Montet (Chicago),
  Hans-Walter Rix (MPIA),
  Julian Stuermer (Chicago),
and the participants in the weekly Stars Group Meeting at the Flatiron
Institute for valuable contributions to this project.
None of this would have been possible without the incredible effort
of the \HARPS\ team and \acronym{ESO}, who built the instrument and made the
data used here public.
MB thanks the Max-Planck-Institut f\"ur Astronomie in Heidelberg
for hospitality during a critical phase of this project.

\todo{...add Hogg grant numbers}

\todo{...add facilities tags}

\todo{...add software tags}

\bibliographystyle{apj}
\bibliography{}%general,myref,inprep}

\end{document}
