\documentclass[modern]{aastex61}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[sort&compress]{natbib}
\usepackage[hang,flushmargin]{footmisc}

\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\m}{\unit{m}}
\newcommand{\s}{\unit{s}}
\newcommand{\kms}{\km\,\s^{-1}}
\newcommand{\ms}{\m\,\s^{-1}}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\HARPS}{\project{\acronym{HARPS}}}
\newcommand{\RV}{\acronym{RV}}
\newcommand{\CRLB}{\acronym{CRLB}}
\newcommand{\EPRV}{\acronym{EPRV}}
\newcommand{\dd}{\mathrm{d}}

\begin{document}
\graphicspath{ {figures/} }
\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{Achieving maximum possible precision on stellar radial-velocity measurements in multi-epoch spectroscopy}

\author{Megan Bedell}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affiliation{University of Chicago, NEED DETAILS HERE}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}
% context
Extremely precise stellar radial velocity (\RV) measurements are a foundational tool in the field of exoplanets, with hundreds of planets discovered by the Doppler method to date and about a dozen high-resolution spectrographs around the world dedicated to the search.
Despite the importance of these observations, the detailed computational methods used to extract $\ms$-level \RV s from stellar spectra remain largely unstandardized and unpublished.
% aims
In this work, we look at sensible approaches to extracting \RV s from a data set of extracted spectra and compare their efficiencies by testing on a segment of realistic model spectrum, with the goal of identifying methods that have the possibility of saturating the information-theoretic Cram\'er--Rao bound (\CRLB) on measurement precision.
% methods
We consider cross-correlations (and, equivalently, maximum-likelihood optimizations) of the data with generalized (weighted) binary masks, synthetic spectra (from, say, stellar models), and data-driven templates constructed from the data themselves.
We also consider the possibility that there are low-amplitude, unknown telluric absorptions (micro-tellurics) affecting the spectrum, and continuum-normalization problems, and measure improvements that flow from modeling those also in a data-driven way.
% results
We find that of all methods, only cross-correlation with an iteratively built data-driven template comes close to saturating the \CRLB.
We find that if there are micro-tellurics, they can be captured by a data-driven model.
We predict that simultaneously fitting data-driven models of the stellar spectrum and telluric contributions at all observational epochs will improve the \RV\ precision of \HARPS\ to substantially better than $1\,\ms$.
\end{abstract}

\section{Introduction}

Accurate spectroscopic measurements of radial velocities (\RV s) have long been a critical tool across many subfields of astronomy, from galactic dynamics to cosmic expansion. In recent decades, however, a new focus has emerged on \textit{extremely precise} \RV\ measurements of stars as a means of detecting exoplanets. For this application, measuring the absolute velocity of the star is much less important than resolving its changes through time. With sub-$\ms$-level precision required for the detection of Earth-like planets, vast amounts of effort have been poured into the engineering challenges of constructing a sufficiently stable spectrograph (or, failing that, being able to track the drift of a spectrograph over time) \todo{(cite some review papers)}. At the same time, great strides have been made towards disentangling true Doppler shifts from imitative signals like the spectral signatures of stellar pulsations and magnetic activity features \todo{(cite some review papers)}.  In this work, we investigate another fundamentally important but less commonly discussed aspect of the quest for maximum-precision \RV s: the method of analysis used to extract the \RV\ from a stellar spectrum.

While the general idea of measuring a Doppler shift by the relative motions of stellar absorption lines is fairly straightforward, there are considerable subtleties to implementing this, from the choice of reference spectrum or template to the treatment of telluric absorption features. Ultra-stabilized spectrographs like HARPS \todo{(define \& cite)} predominantly utilize a cross-correlation method with a custom-built list of spectral lines (see e.g. Queloz1995, Baranne1996). A lesser-used alternative is cross-correlation or maximum-likelihood fitting using a template spectrum generated from the data (Buchhave, Anglada-Escude). This often involves masking or downweighting of the most telluric-contaminated spectral regions.

On the other hand, gas-cell-calibrated spectrographs like HIRES and [whatever the McDonald one is] commonly adopt a more comprehensive forward-modeling approach. In this framework, the instrumental line spread function, the stellar RV, and other parameters are optimized by modeling each observation as a combination of shifted and instrument-convolved template spectra for the star and gas cell. [say a bit more]

Our primary aim in this work is to establish a common frame of reference for these methods based on their relative ability to extract maximum-precision RVs and their robustness in the face of various expected structured noise sources.

We begin in Section \ref{s:info} with a derivation of the information-theory-based limit on the RV information content of a spectrum. In Section \ref{s:methods}, we summarize common RV extraction practices used by current \RV\ pipelines, including cross-correlation and template fitting. We then implement these techniques to extract \RV s using 5-\r{A} section of simulated and real spectra, as described in Section \ref{s:data}. We compare the relative performances of the methods in Section \ref{s:experiments} and conclude with some recommendations for future RV pipelines in Section \ref{s:summary}. 

It is important to state at the outset the assumptions under which are operating for these experiments. We assume that our data set consists of multiple observations of the same star with good coverage throughout the observing season, so that we have seen the star many times spanning the range of telluric-to-star relative RV shifts. This means we are not considering the case of a real-time data reduction pipeline. We also assume a HARPS-like setup, i.e. no absorption cell, but the points we make can be easily generalized to the gas-cell case. We return to this point in the discussion \todo{(do this)}.

We additionally assume that the data are extremely well wavelength-calibrated. In other work, we will discuss this, but our current belief is that current RV spectrographs are supremely well wavelength-calibrated.

[any other assumptions?]

\section{Information Theory}
\label{s:info}

Given finite, noisy data, there are limits coming from information
theory on how well any parameter of interest can be measured.
The most straightforward bound is the Cram\'er--Rao Lower Bound (\CRLB) on the
variance of any unbiased frequentist estimator.
Estimators can only beat this bound by taking on bias.

This bound is strong, but it cannot be calculated \emph{objectively};
any description of the information in a data set requires
subjective decisions (that is, debatable decisions), which are inputs
to a model for the generation of the data.
In particular, information-theory bounds require a model for the
probability of the data given the parameter of interest; that is, they
require a likelihood function.
(Side note: Bayesians need a likelihood function for inference, and
frequentists need a likelihood function for analyzing their estimators.
The likelihood function is central to all statistical methods.)

In particular, the \CRLB\ is related to the Fisher Information that is
frequently used in cosmological experiment forecasts (\todo{CITE
  SHIT}).
It is that the \emph{inverse variance} $\sigma_v^{-2}$ of any velocity
estimator must be smaller than the second derivative of the
\emph{negative} log likelihood function:
\begin{eqnarray}
\frac{1}{\sigma_v^2} &\geq& \frac{\dd^2}{\dd v^2}(-\ln L)
\end{eqnarray}

In the case of \EPRV, this says that any computation of the limits
on estimating stellar velocities (or stellar velocity changes)
require making assumptions about the stellar spectrum (the
latent, unobserved, \emph{true} spectrum that could only be observed
with arbitrarily good data) and the noise processes that distort the
spectrum in any finite, noisy observation.
These noise processes ought (in principle) to include any beliefs about
unsubtracted telluric absorptions or emissions, and time variability
of the star itself, not to mention photon and spectrograph read and
extraction noise.
However---and in keeping with the literature---we can make maximal or
best-case assumptions, and ask what we would get for \EPRV\ precision
in that best case.
Then we can ask (with fake data) how much different choices about our
data analysis, and different adversarial injections of noise,
tellurics, spectrograph issues, and other wrongness can prevent us
from achieving the best-case bounds.

The most optimistic assumptions are... Gaussian... No
tellurics... perfect calibration... no time dependence... all
variances known...

\begin{itemize}
\item the Cramer-Rao bound is the limit on measurement
\item it relates to Fisher information
\item it cannot be computed without a model
\item CRLB is saturated with a maximum likelihood estimator under the correct model
\item wrong spectrum template = wrong and noisy \RV s
\end{itemize}

\begin{itemize}
\item what is the Cramer-Rao bound on a spectrum? See notes in Hogg's red book for 2017-12-01.
\item Butler equation
\item Bouchy equation
\item Figueira (or Lovis \& Fischer) equation
\end{itemize}

In conclusion: the saturate the CRLB, we need to construct a very accurate spectral template, an accurate tellurics template, and maximize the likelihood. Given that theory sucks, this probably means data-driven.

\section{Overview of Methods}
\label{s:methods}

cross-correlation and max likelihood are the same: here's why. We are going to consider cross-correlation for simplicity in this paper.

for either of these, you could use:
\begin{itemize}
\item ``binary'' mask: reasons you might want to use this (automatically ignores tellurics, trivially computes average line profile), pipelines that use this \& how (quote HARPS papers)
\item synthetic spectrum (i.e. a slightly wrong model prediction, since the stellar model will never be a perfect reproduction of the star in question): sub-tweet: some surveys cross-correlate with a single (same) template for every single star, which is extremely wrong. you can be more right by using a spectral-type-specific template.
\item data-driven template spectrum: some surveys (HIRES) use a single observation as a template. combining information from many visits is better because the combined SNR is amazing.
\end{itemize}


discuss quadratic max procedure to get a fine-grained maximum from a coarse-grained cross-correlation grid


\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{binarymask}
\caption{Illustration of a weighted binary mask (blue windows) plotted over a small segment of the solar spectrum (black). The vertical length of each mask window corresponds to the weight given to the line when performing a cross-correlation between mask and spectrum.}
\label{fig:binarymask}
\end{figure}

\section{Data}
\label{s:data}

Most of what we do in this paper uses artificial data because we need access to the truth in order to compute the theoretical information content of the data. As a final test, we do use real data as a demonstration that the methods presented here are useful when applied to real data and the assumptions made in our artificial data generation are realistic.

\subsection{Artificial Data}

The basic model that we use to generate fake spectra is extremely simple, assuming a perfectly normalized continuum; isolated, perfectly Gaussian spectral lines; and white noise only. We simulated a 5-$\AA$ region with random noise corresponding to an SNR of 100, a typical value for a single \RV\ spectrum. Lines were inserted as Gaussians with arbitrarily-prescribed centers and depths (or equivalent widths). Every line was convolved with the same line spread function, which was taken to be a Gaussian with $\sigma$ = 0.05 $\AA$. %The number of lines used varies in the results discussed below.

We repeated this synthesis to make a set of \todo{64} spectra, each with a random \RV\ between $-30$ and $30\,\kms$ (the approximate amplitude of the yearly \RV\ shifts induced by barycentric motion). The \RV s can be applied as a straightforward Doppler shift in the centers of each line. \todo{(add equation?)} At this point, we are able to run a cross-correlation or a maximum-likelihood analysis on the set of spectra and examine the deviations in the recovered \RV s from the input \RV s.

discuss single-line and multi-line cases

FIGURE showing multi-line artificial data

\subsection{HARPS Data}

munging notes

FIGURE showing analogous section of real data

\section{Experiments}
\label{s:experiments}

\subsection{Single-Line}

We began with the simplest test case possible: a single, strong absorption feature.

What is the CRLB?

Discuss scaling of the CRLB with line FWHM, constrast/depth, etc. According to Lovis \& Fischer (2010) and Figueira (2017), $\sigma_{RV}$ should go as $\frac{\sqrt{FWHM}}{SNR \times C}$, where $C$ is the line contrast, or the depth of the line as a fraction of the continuum level \todo{(check on this definition of contrast)}. Do we see this?

\subsection{Binary Mask}

Our next test involved a more realistic test spectrum. We measured the centers and depths of \todo{N} lines in the Solar spectrum between {x - y} $\AA$ and used these measurements to generate a synthetic spectrum that closely resembles a typical observation of a Sun-like star.

What is the CRLB for this case? Binary mask cross-correlation is very far from saturating it. Cross-correlation or max-likelihood fitting with a perfect spectral template works much better. Talk a little bit about why.

\subsection{Rigid Template}

how well does the mask/template approximate the real data? try fudging the relative line depths and locations in both the binary mask and the template. do a correct template and an incorrect template.

even a tiny wrongness to the template is visible in the RV variance.

\subsection{Data-Driven Template}

how do we construct the template? show that it comes close to saturating the CRLB.

comment: there are far better ways to derive the data-driven template. we leave this to future work.

\subsection{Realistic Sources of Structured Noise}

additional noise considerations:
\begin{itemize}
\item (micro-) telluric features: tellurics that you can't identify are important
\item subtle continuum trends: not that big a deal
\item cosmic rays?: we might skip this and add to discussion
\item additive signals?: for example sky emission, lunar and other reflected solar; we might skip this and just add to discussion
\end{itemize}

\subsection{HARPS Data}

all the data-driven code works on real data. we get a better RMS on the star from the data-driven template than from the binary mask. based on this, we believe that the HARPS pipeline doesn't strictly use a binary mask.

\section{Summary \& Discussion}
\label{s:summary}

paragraph for each major result from the abstract

wrongness of our assumptions - how does this affect our conclusions?

limitations of our ``additional noise'' ideas and what we have considered.

Hogg: insert some grand final statement.

\acknowledgements
MPIA hospitality, Montet, Foreman-Mackey, Bean, Stuermer, Rix, HARPS team, ESO, Stars group meeting, Brewer

\bibliographystyle{apj}
\bibliography{}%general,myref,inprep}

\end{document}
