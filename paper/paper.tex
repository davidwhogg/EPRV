\documentclass[modern]{aastex61}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[sort&compress]{natbib}
\usepackage[hang,flushmargin]{footmisc}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\m}{\unit{m}}
\newcommand{\s}{\unit{s}}
\newcommand{\kms}{\km\,\s^{-1}}
\newcommand{\ms}{\m\,\s^{-1}}
\newcommand{\ang}{\text{\normalfont\AA}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}

% text macros
\newcommand{\sectionname}{Section}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\HARPS}{\project{\acronym{HARPS}}}
\newcommand{\HIRES}{\project{\acronym{HIRES}}}
\newcommand{\RV}{\acronym{RV}}
\newcommand{\CRLB}{\acronym{CRLB}}

\setlength{\parindent}{1.4em} % trust in Hogg
\shorttitle{achieving maximum radial-velocity precision}
\shortauthors{bedell and hogg}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
\graphicspath{ {figures/} }
\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{Achieving maximum possible precision on stellar radial-velocity measurements in multi-epoch spectroscopy}

\author[0000-0001-9907-7742]{Megan Bedell}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Department of Astronomy \& Astrophysics, University of Chicago, 5640 S. Ellis Ave, Chicago, IL 60637, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Extremely precise stellar radial velocity (\RV) measurements are a foundational tool in the field of exoplanets, with hundreds of planets discovered by the Doppler method to date and about a dozen high-resolution spectrographs around the world dedicated to the search.
Despite the importance of these observations, the detailed computational methods used to extract $\ms$-level \RV s from stellar spectra remain largely unstandardized and unpublished.
% aims
In this work, we look at sensible approaches to extracting \RV s from a data set of extracted spectra and compare their efficiencies by testing on a segment of realistic model spectrum, with the goal of identifying methods that have the possibility of saturating the information-theoretic Cram\'er--Rao bound (\CRLB) on measurement precision.
% methods
We consider cross-correlations (and, equivalently, maximum-likelihood optimizations) of the data with generalized (weighted) binary masks, synthetic spectra (from, say, stellar models), and data-driven templates constructed from the data themselves.
We also consider the possibility that there are low-amplitude, unknown telluric absorptions (micro-tellurics) affecting the spectrum, and continuum-normalization problems, and measure improvements that flow from modeling those also in a data-driven way.
% results
We find that of all methods, only cross-correlation with an iteratively built data-driven template comes close to saturating the \CRLB.
We find that if there are micro-tellurics, they can be captured by a data-driven model.
We predict that simultaneously fitting data-driven models of the stellar spectrum and telluric contributions at all observational epochs will improve the \RV\ precision of \HARPS\ to substantially better than $1\,\ms$.
\end{abstract}

\keywords{
atmospheric effects
---
binaries: spectroscopic
---
methods: data analysis
---
methods: statistical
---
techniques: radial velocities
---
planets and satellites: detection
}

\section{Introduction}


Accurate spectroscopic measurements of radial velocities (\RV s) have long been a critical tool across many subfields of astronomy, from Galactic dynamics to large-scale structure and cosmic expansion.
Fundamentally, accuracy of radial-velocity mesurement is limited by the quality of spectral models of stars, or templates.
In recent decades, however, a new focus has emerged on \emph{extremely precise} but purely \emph{relative} \RV\ measurements of stars as a means of detecting exoplanets and other kinds of companions.
For this application, measuring the absolute velocity of the star is much less important than resolving changes through time.
With sub-$\ms$-level precision required for the detection of Earth-like planets, vast amounts of effort have been poured into the engineering challenges of constructing a sufficiently stable spectrograph (or, failing that, being able to track the drift of a spectrograph over time) \todo{(cite some review papers)}. At the same time, great strides have been made towards disentangling true Doppler shifts from imitative signals like the spectral signatures of stellar pulsations and magnetic activity features \todo{(cite some review papers)}.  In this work, we investigate another fundamentally important but less commonly discussed aspect of the quest for maximum-precision \RV s: the method of analysis used to estimate the \RV\ given an extracted stellar spectrum.

While the general idea of measuring a Doppler shift by the relative motions of stellar absorption lines is fairly straightforward, there are considerable subtleties to implementing this, from the choice of reference spectrum or template to the treatment of telluric absorption features. Ultra-stabilized spectrographs like \HARPS\ \todo{(define \& cite)} predominantly utilize a cross-correlation method with a custom-built list of spectral lines (see e.g. Queloz1995, Baranne1996). 
A lesser-used alternative is cross-correlation or maximum-likelihood fitting using a template spectrum generated from the data (Buchhave, Anglada-Escude). 
This often involves masking or downweighting of the most telluric-contaminated spectral regions.

On the other hand, gas-cell-calibrated spectrographs like \HIRES\ commonly adopt a more comprehensive forward-modeling approach. 
In this framework, the instrumental line spread function, the stellar RV, and other parameters are optimized by modeling each observation as a combination of shifted and instrument-convolved template spectra for the star and gas cell. [say a bit more]

Our primary aim in this work is to establish a common frame of reference for these methods based on their relative ability to extract maximum-precision RVs and their robustness in the face of various expected structured noise sources.

For the purposes of this work, we assume that our data set consists of multiple observations of the same star with good coverage throughout the observing season, so that we have seen the star many times spanning the range of telluric-to-star relative RV shifts. 
This means we are not considering the case of a real-time data reduction pipeline. 
We also assume a \HARPS-like setup, i.e. no absorption cell, but the points we make can be easily generalized to the gas-cell case. 
We return to this point in the discussion \todo{(do this)}.

We begin in Section \ref{s:info} with a derivation of the information-theory-based limit on the RV information content of a spectrum. 
In Section \ref{s:methods}, we summarize common RV extraction practices used by current \RV\ pipelines, including cross-correlation and template fitting. 
We then implement these techniques to extract \RV s using 5-\ang-wide sections of simulated and real spectra, as described in Section \ref{s:data}. 
We compare the relative performances of the methods in Section \ref{s:experiments} and conclude with some recommendations for future RV pipelines in Section \ref{s:summary}. 



\section{Information Theory}
\label{s:info}

Given finite, noisy data, there is a limit coming from information
theory on how well any parameter of interest can be measured.
This limit is
sometimes known as the Cram\'er--Rao Lower Bound (\CRLB; \todo{cite CR and wikipedia}).
In its simplest form, it constrains the variance of any unbiased
frequentist estimator.
Estimators can only beat this bound by taking on bias.

This bound is strong, but it cannot be calculated \emph{objectively};
any description of the information in a data set requires
subjective decisions (that is, debatable decisions), which are inputs
to a model for the generation of the data.
In particular, information-theory bounds require a model for the
probability of the data given the parameter of interest; that is, they
require a likelihood function.
As an aside: Bayesians need a likelihood function for inference, and
frequentists need a likelihood function for analyzing their estimators.
The likelihood function is therefore central to all statistical methods,
and the core component of any data analysis.

In particular, the \CRLB\ is related to the Fisher Information that is
frequently used in cosmological experiment forecasts (\todo{CITE
  SHIT}).
It is that the \emph{inverse variance} $\sigma_v^{-2}$ of any velocity
estimator (that is, the information in the velocity estimator) must be smaller than the
second derivative of the \emph{negative} log likelihood function:
\begin{equation}
\frac{1}{\sigma_v^2} \leq E\left[\frac{\dd^2}{\dd v^2}(-\ln L)\right] \quad,
\end{equation}
where $L$ is the likelihood, or the probability of the data given the velocity,
and the expectation $E[\cdot]$ is taken over all possible data (that is, it is an
expectation under the likelihood).
If there are many other nuisance parameters, a Bayesian can marginalize them
out to make $L$ the marginalized likelihood for the velocity, or a frequentist
can include them in the estimation and compute an inverse covariance matrix $C^{-1}$
for the full parameter vector, which will have the bound
\begin{equation}
v^T\cdot C^{-1}\cdot v \leq v^T\cdot Q\cdot v
\end{equation}
\begin{equation}
Q_{ij} \equiv E\left[\frac{\dd^2}{\dd\theta_i\,\dd\theta_j}(-\ln L)\right] \quad,
\end{equation}
where $Q$ is the information tensor, $i,j$ are indices into that
tensor, corresponding parameters are $\theta_i, \theta_j$, and $v$ is
any arbitrary vector.  That is, the bound is not just on every
parameter, but really on on any combination of parameters.

In the case of extreme-precision \RV, this says that any computation of the limits
on estimating stellar velocities (or stellar velocity changes)
requires a likelihood function.
This, in turn, requires making assumptions about the stellar spectrum (the
latent, unobserved, \emph{true} spectrum that could only be observed
with arbitrarily good data) and the noise processes that distort the
spectrum in any finite, noisy observation.
These noise processes ought (in principle) to include any beliefs about
unsubtracted telluric absorptions or emissions, and time variability
of the star itself, not to mention photon and spectrograph read and
extraction noise.
However---and in keeping with the literature---we can make maximal or
best-case assumptions, and ask what we would get for \RV\ precision
in that best case.
Then we can ask (with fake data) how much different choices about our
data analysis, and different adversarial injections of noise,
tellurics, spectrograph issues, and other wrongness can prevent us
from achieving the best-case bounds.

The most optimistic assumptions we can possibly make are the following.
\begin{itemize}
\item The spectrograph is perfectly calibrated in a wavelength sense.
\item The spectrograph is consistently and repeatably calibrated in a
  flux sense. That is, the spectrum does not need to be perfectly
  continuum-normalized or perfectly flux-normalized, but it is
  calibrated near-perfectly \emph{consistently} across epochs (exposures).
\item There are no residual telluric absorptions nor any residual sky
  emission in the spectra. That is, the sky is near-perfectly calibrated
  and removed.
\item There is no time dependence (no epoch-to-epoch variations) in the
  stellar spectrum.
\item The \emph{true} spectrum (that is, the latent spectrum that
  would be observed if the data were far, far better) of the star is
  known at better accuracy and precision than the main noise sources.
\item All noise sources contributing to the spectral data---which
  include at a minimum photon noise, read noise, residual telluric
  issues, residual wavelength-calibration issues, and residual stellar
  spectral variability---are, when summed together, indistinguishable
  from being zero-mean, Gaussian, and additive.
\item The complete variance tensor of the total noise on the spectrum is known, and
  known correctly. That is, the variance of the noise at every pixel
  is known, as is the covariance between nearby pixels, and in the
  same \emph{true} sense that the true spectrum of the star is known.
\end{itemize}
It is worthy of note that every formula (that we know) in the
literature for maximum \RV\ precision (\todo{cite Butler, Bouchy,
  Figueira, Lovis}) makes these same assumptions, plus additional \emph{even
more restrictive assumptions}.
Under these disturbingly unrealistic assumptions, the \CRLB\ becomes
\begin{equation}\label{eq:crlb}
\frac{1}{\sigma_v^2} \leq \left[\frac{\dd f}{\dd v}\right]\T\cdot C^{-1}\cdot\left[\frac{\dd f}{\dd v}\right]
\end{equation}
where the derivatives are of the true spectrum with respect to
velocity (which can be thought of as the derivative of a Doppler
operator with respect to velocity acting on the true spectrum), the
derivatives are column vectors, and $C^{-1}$ is the inverse of the covariance matrix
describing the true variance and covariances of the noise process.

Formulae (bounds) related to (\ref{eq:crlb}) have appeared previously
in the literature, although in somewhat more complicated forms.
Butler (\todo{cite}) shows a similar expression, but makes the additional
assumption that every pixel is independent; that is, that there is no
covariance between nearby pixels from, say, extraction or continuum normalization.
Bouchy (\todo{cite}) shows an expression that assumes that the pixels
are independent and that the noise is dominated by a combination of photon
and read noise. 
That work also replaces the model spectrum with a set of optimal
weights for cross-correlation, which amounts to very much the same thing.
Figueira (\todo{cite}) and \todo{others (cite)} show a per-line
bound, based on assumptions similar to those of Bouchy but with the additional assumption that
each line is well approximated by a Gaussian (squared exponential) in shape.
These literature bounds are all extremely similar, and similar to (\ref{eq:crlb});
nothing here is controversial.

Now that we have established the bound, how can we saturate it? That is,
how can we make measurements (estimators)  with variances that come close to the \CRLB?
The short answer is that reliable bound-saturating estimators are maximum-likelihood estimators.
The long answer is long, but part of it is that the bound can only be saturated if the
assumptions are correct, and note that the assumptions include that we know the
\emph{true} spectrum of the star, among many other things.
That is, if we can take data that come close to meeting the assumptions, and
if we can find a spectral model that comes close to the true spectrum of the star,
then in principle we can estimate radial velocities with a precision close to the \CRLB.

Given that computational stellar spectral models have physical issues, the best way
to get an accurate estimate of the true spectrum of the star is going to be data-driven.
In what follows, we are going to show that---for a typical \HARPS-like observing program---%
it is possible to build a data-driven model of the stellar spectrum that is good enough
to nearly-saturate the \CRLB.
That is, with a data-driven model of the true stellar spectrum, model inaccuracy will
not dominate the \RV\ noise budget.
We will additionally bring evidence that telluric features can be a significant problem,
but that they also can plausibly be mitigated with a data-driven model.

\section{Radial-velocity estimators}
\label{s:methods}

Most methods for radial-velocity estimation involve some kind of
cross-correlation of the data with a template.
This might be surprising, since (as we noted above) bound-saturating
estimators will be maximum-likelihood estimators.
However, under some simple assumptions, cross-correlation optima are
also maximum-likelihood optima.

For example, when the likelihood is Gaussian, the log-likelihood
contains a chi-squared-like term
\begin{equation}
-\frac{1}{2}\,[y - f]\T\cdot C^{-1}\cdot [y - f] \quad ,
\end{equation}
where $y$ is a column vector of the data, $f$ is the model spectrum,
and $C$ is the covariance matrix describing the noise variance.
In the case of independent pixel noise this just becomes an
inverse-variance weighted sum of squared residuals (chi-squared).
If we expand this quadratic form, it becomes
\begin{equation}
{}-\frac{1}{2}\,y\T\cdot C^{-1}\cdot y + y^T\cdot C^{-1}\cdot f 
-\frac{1}{2}\,f\T\cdot C^{-1}\cdot f \quad.
\end{equation}
That is, the chi-squared term factors into a term that is the square
of the model, a term that is the square of the data, and a term that
is a cross-correlation.
As long as the amplitude of $f$ doesn't depend on velocity, and the
$C$ tensor is ``smooth'' in certain respects, the dependence of this
chi-squared term on velocity will be dominated by the
cross-correlation.
For this reason, we will use a variant of cross-correlation
optimization as our radial-velocity estimator.

...In detail, the cross-correlation we will optimize is...

A variety of choices exist for the cross-correlation template $f$. A few possibilities include:
\begin{itemize}
\item a ``binary'' mask, or a spectrum of zero value with non-zero peaks at the expected rest-frame wavelengths of the spectral lines of interest. A cross-correlation between this mask and the observed spectrum will effectively compute the average line profile with its minimum corresponding to the best-fit RV. This method has the benefit of trivially excluding telluric-contaminated or otherwise unreliable regions of the spectrum by deleting them from the mask. A desired relative weighting of lines can be incorporated by setting the non-zero mask values to different amplitudes at different lines (cite Pepe). This method is currently used by the \HARPS\ pipeline (cite).
\item synthetic spectrum (i.e. a slightly wrong model prediction, since the stellar model will never be a perfect reproduction of the star in question): sub-tweet: some surveys cross-correlate with a single (same) template for every single star, which is extremely wrong. you can be more right by using a spectral-type-specific template.
\item data-driven template spectrum: some surveys (HIRES) use a single observation as a template. combining information from many visits is better because the combined SNR is amazing.
\end{itemize}

One final implementation detail is the following:
When we optimize the cross-correlation function, we don't run an optimizer;
we perform brute-force search on a linear velocity grid.
This grid, however, does not need to be fine.
We use a coarse grid (\todo{give grid spacing here}), which does not
over-resolve the velocity peak at the optimum.
We then take the highest value on the grid and the values on either
side, fit a parabola through those three values, and take as the optimal
velocity the peak of that parabola.
That is, we obtain sub-grid velocity resolution by means of a quadratic
interpolation at the peak on the finite grid.
\todo{Do we need to demonstrate this with a toy figure?}

\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{binarymask}
\caption{Illustration of a weighted binary mask (blue windows) plotted over a small segment of the solar spectrum (black). The vertical length of each mask window corresponds to the weight given to the line when performing a cross-correlation between mask and spectrum.}
\label{fig:binarymask}
\end{figure}

\section{Data}
\label{s:data}

The experiments we perform in this paper primarily use artificial data, for the simple reason that we require access to the underlying true RVs in order to assess the performance of our methods. 
We attempt to generate the fake data as realistically as possible for the case of a high-resolution, high-signal-to-noise spectrum of a Sun-like star taken with \HARPS. 
With these data, we are able to compute the \CRLB.  
We can also isolate the effects of various confounding noise sources like telluric features and imperfect continuum normalization by injecting them into the artificial data, which is generated free of correlated noise by default.

We will also show tests on real \HARPS\ data for two stars, one hosting a companion and
one (apparently) companion-free (at least at our precision). 
These tests serve as a demonstration that the methods presented here are useful when applied to real data and the assumptions made in our artificial data generation are realistic.

\subsection{Artificial Data}

The basic model that we use to generate fake spectra is extremely simple, assuming a perfectly normalized continuum; isolated, perfectly Gaussian spectral lines; and white noise only. 
We simulated a 5-\ang\ region with random noise corresponding to an SNR of 100, a typical value for a single \RV\ spectrum. 
Lines were inserted as Gaussians with prescribed centers, widths, and depths. 
Every line was convolved with the same line spread function, which was taken to be a Gaussian with $\sigma$ = 0.05 \ang. 
In the simplest test case, an arbitrary single line at 5000 \ang\ was used (see Section \ref{s:singleline}). 
However, for most of our tests we injected a set of 12 lines with centers between 4998 - 5002 \ang. 
The Gaussian parameters for these lines were based on fits to a reflected Solar spectrum from Vesta taken by \HARPS. \todo{(find citation/program number)} 
The resulting spectrum strongly resembles a 5-\ang\ chunk taken from a typical \HARPS\ spectrum of a Sun-like star (Figure \ref{fig:spectra}).

We repeated this synthesis to make a set of \todo{64} spectra, each with a random \RV\ between $-30$ and $30\,\kms$ (the approximate amplitude of the yearly \RV\ shifts induced by barycentric motion). 
The \RV s were applied as a multiplicative Doppler shift on the central wavelength of each spectral line to be injected. 
At this point, we are able to run a cross-correlation analysis on the set of spectra and examine the deviations in the recovered \RV s from the input \RV s.

\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{spectra}
\caption{}
\label{fig:spectra}
\end{figure}

\subsection{\HARPS\ Data}

munging notes

reference to figure comparing real \& fake data

\section{Experiments and results}
\label{s:experiments}

\subsection{Single-Line}
\label{s:singleline}

We began with the simplest test case possible: a single, strong absorption feature.

What is the CRLB?

Discuss scaling of the CRLB with line FWHM, constrast/depth, etc. According to Lovis \& Fischer (2010) and Figueira (2017), $\sigma_{RV}$ should go as $\frac{\sqrt{FWHM}}{SNR \times C}$, where $C$ is the line contrast, or the depth of the line as a fraction of the continuum level \todo{(check on this definition of contrast)}. Do we see this?

\subsection{Binary Mask}

Our next test involved a more realistic test spectrum. We measured the centers and depths of \todo{N} lines in the Solar spectrum between {x - y} $\ang$ and used these measurements to generate a synthetic spectrum that closely resembles a typical observation of a Sun-like star.

What is the CRLB for this case? Binary mask cross-correlation is very far from saturating it. Cross-correlation or max-likelihood fitting with a perfect spectral template works much better. Talk a little bit about why.

\subsection{Rigid Template}

how well does the mask/template approximate the real data? try fudging the relative line depths and locations in both the binary mask and the template. do a correct template and an incorrect template.

even a tiny wrongness to the template is visible in the RV variance.

\subsection{Data-Driven Template}

how do we construct the template? show that it comes close to saturating the CRLB.

comment: there are far better ways to derive the data-driven template. we leave this to future work.

\subsection{Realistic Sources of Structured Noise}

additional noise considerations:
\begin{itemize}
\item (micro-) telluric features: tellurics that you can't identify are important
\item subtle continuum trends: not that big a deal
\item cosmic rays?: we might skip this and add to discussion
\item additive signals?: for example sky emission, lunar and other reflected solar; we might skip this and just add to discussion
\end{itemize}

\subsection{Real Data}

all the data-driven code works on real data. we get a better RMS on the star from the data-driven template than from the binary mask. based on this, we believe that the \HARPS\ pipeline doesn't strictly use a binary mask.

\section{Summary and discussion}
\label{s:summary}

...paragraph for each major result from the abstract

In \sectionname~\ref{s:info} we listed some exceedingly
optimistic assumptions under which we could compute the
information-theoretic limit on radial-velocity precision.
Some of these assumptions are worthy of discussion: How true are they,
and what happens, both from an information-theory perspective and from
a methodological perspective, when we relax them?

One of these assumptions, for example, is that the true stellar
spectrum is constant with time; that is, the variations from
epoch-to-epoch are from noise or nuisances (like telluric absorption).
This assumption is critical and deep.
The entire edifice of extreme-precision \RV\ is built on this assumption:
With imperfect models of stellar structure and photospheres, it is
impossible to know the \emph{absolute} \RV\ of a star to high precision.
The reason is that small issues with (say) convective line shifts or
line-shapes, it is impossible to know precisely the true wavelength of
a line centroid, unless the model is correct in an absolute sense.
So we satisfy ourselves that we can measure \emph{relative} velocities
to arbitrarily high precision:
Relative precision---and everything in this paper---depends on our
ability to measure shifts from epoch to epoch.
If the stellar spectrum is permitted (or observed) to change from epoch
to epoch, there is no fundamental way to determine or confirm spectral
shifts corresponding to Doppler shifts.
That is, even relative \RV\ measurements depend on a constancy of the
stellar spectrum.
This is a deep issue, extending far beyond the scope of this paper;
time-variable stellar spectra are not killing to the extreme-precision
\RV\ program!
But they do require substantial and non-trivial adjustments to the
\foreign{status quo}.
We hope to address these in future work.


...MOAR assumptions

...limitations of our ``additional noise'' ideas and what we have considered.

...Hogg: insert some grand final statement.

\acknowledgements
MPIA hospitality, Montet, Foreman-Mackey, Bean, Stuermer, Rix, \HARPS\ team, ESO, Stars group meeting, Brewer

\bibliographystyle{apj}
\bibliography{}%general,myref,inprep}

\end{document}
