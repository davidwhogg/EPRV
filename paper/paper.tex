\documentclass[modern]{aastex61}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[sort&compress]{natbib}
\usepackage[hang,flushmargin]{footmisc}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\m}{\unit{m}}
\newcommand{\s}{\unit{s}}
\newcommand{\kms}{\km\,\s^{-1}}
\newcommand{\ms}{\m\,\s^{-1}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}

% text macros
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\HARPS}{\project{\acronym{HARPS}}}
\newcommand{\RV}{\acronym{RV}}
\newcommand{\CRLB}{\acronym{CRLB}}
\newcommand{\EPRV}{\acronym{EPRV}}

\setlength{\parindent}{1.4em} % trust in Hogg
\shorttitle{achieving maximum radial-velocity precision}
\shortauthors{bedell and hogg}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
\graphicspath{ {figures/} }
\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{Achieving maximum possible precision on stellar radial-velocity measurements in multi-epoch spectroscopy}

\author[0000-0001-9907-7742]{Megan Bedell}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affiliation{Department of Astronomy \& Astrophysics, University of Chicago, 5640 S. Ellis Ave, Chicago, IL 60637, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Extremely precise stellar radial velocity (\RV) measurements are a foundational tool in the field of exoplanets, with hundreds of planets discovered by the Doppler method to date and about a dozen high-resolution spectrographs around the world dedicated to the search.
Despite the importance of these observations, the detailed computational methods used to extract $\ms$-level \RV s from stellar spectra remain largely unstandardized and unpublished.
% aims
In this work, we look at sensible approaches to extracting \RV s from a data set of extracted spectra and compare their efficiencies by testing on a segment of realistic model spectrum, with the goal of identifying methods that have the possibility of saturating the information-theoretic Cram\'er--Rao bound (\CRLB) on measurement precision.
% methods
We consider cross-correlations (and, equivalently, maximum-likelihood optimizations) of the data with generalized (weighted) binary masks, synthetic spectra (from, say, stellar models), and data-driven templates constructed from the data themselves.
We also consider the possibility that there are low-amplitude, unknown telluric absorptions (micro-tellurics) affecting the spectrum, and continuum-normalization problems, and measure improvements that flow from modeling those also in a data-driven way.
% results
We find that of all methods, only cross-correlation with an iteratively built data-driven template comes close to saturating the \CRLB.
We find that if there are micro-tellurics, they can be captured by a data-driven model.
We predict that simultaneously fitting data-driven models of the stellar spectrum and telluric contributions at all observational epochs will improve the \RV\ precision of \HARPS\ to substantially better than $1\,\ms$.
\end{abstract}

\keywords{
atmospheric effects
---
binaries: spectroscopic
---
methods: data analysis
---
methods: statistical
---
techniques: radial velocities
---
planets and satellites: detection
}

\section{Introduction}


Accurate spectroscopic measurements of radial velocities (\RV s) have long been a critical tool across many subfields of astronomy, from Galactic dynamics to large-scale structure and cosmic expansion.
Fundamentally, accuracy of radial-velocity mesurement is limited by the quality of spectral models of stars, or templates.
In recent decades, however, a new focus has emerged on \emph{extremely precise} but purely \emph{relative} \RV\ measurements of stars as a means of detecting exoplanets and other kinds of companions.
For this application, measuring the absolute velocity of the star is much less important than resolving changes through time.
With sub-$\ms$-level precision required for the detection of Earth-like planets, vast amounts of effort have been poured into the engineering challenges of constructing a sufficiently stable spectrograph (or, failing that, being able to track the drift of a spectrograph over time) \todo{(cite some review papers)}. At the same time, great strides have been made towards disentangling true Doppler shifts from imitative signals like the spectral signatures of stellar pulsations and magnetic activity features \todo{(cite some review papers)}.  In this work, we investigate another fundamentally important but less commonly discussed aspect of the quest for maximum-precision \RV s: the method of analysis used to estimate the \RV\ given an extracted stellar spectrum.

While the general idea of measuring a Doppler shift by the relative motions of stellar absorption lines is fairly straightforward, there are considerable subtleties to implementing this, from the choice of reference spectrum or template to the treatment of telluric absorption features. Ultra-stabilized spectrographs like HARPS \todo{(define \& cite)} predominantly utilize a cross-correlation method with a custom-built list of spectral lines (see e.g. Queloz1995, Baranne1996). 
A lesser-used alternative is cross-correlation or maximum-likelihood fitting using a template spectrum generated from the data (Buchhave, Anglada-Escude). 
This often involves masking or downweighting of the most telluric-contaminated spectral regions.

On the other hand, gas-cell-calibrated spectrographs like HIRES and [whatever the McDonald one is] commonly adopt a more comprehensive forward-modeling approach. 
In this framework, the instrumental line spread function, the stellar RV, and other parameters are optimized by modeling each observation as a combination of shifted and instrument-convolved template spectra for the star and gas cell. [say a bit more]

Our primary aim in this work is to establish a common frame of reference for these methods based on their relative ability to extract maximum-precision RVs and their robustness in the face of various expected structured noise sources.

It is important to state at the outset the assumptions under which are operating for these experiments. 
We assume that our data set consists of multiple observations of the same star with good coverage throughout the observing season, so that we have seen the star many times spanning the range of telluric-to-star relative RV shifts. 
This means we are not considering the case of a real-time data reduction pipeline. 
We also assume a HARPS-like setup, i.e. no absorption cell, but the points we make can be easily generalized to the gas-cell case. 
We return to this point in the discussion \todo{(do this)}.

We additionally assume that the data are extremely well wavelength-calibrated. 
In other work, we will discuss this, but our current belief is that current RV spectrographs are supremely well wavelength-calibrated.

[any other assumptions?]

We begin in Section \ref{s:info} with a derivation of the information-theory-based limit on the RV information content of a spectrum. 
In Section \ref{s:methods}, we summarize common RV extraction practices used by current \RV\ pipelines, including cross-correlation and template fitting. 
We then implement these techniques to extract \RV s using 5-\r{A} section of simulated and real spectra, as described in Section \ref{s:data}. 
We compare the relative performances of the methods in Section \ref{s:experiments} and conclude with some recommendations for future RV pipelines in Section \ref{s:summary}. 



\section{Information Theory}
\label{s:info}

Given finite, noisy data, there is a limit coming from information
theory on how well any parameter of interest can be measured.
This limit is
sometimes known as the Cram\'er--Rao Lower Bound (\CRLB; \todo{cite CR and wikipedia}).
In its simplest form, it constrains the variance of any unbiased
frequentist estimator.
Estimators can only beat this bound by taking on bias.

This bound is strong, but it cannot be calculated \emph{objectively};
any description of the information in a data set requires
subjective decisions (that is, debatable decisions), which are inputs
to a model for the generation of the data.
In particular, information-theory bounds require a model for the
probability of the data given the parameter of interest; that is, they
require a likelihood function.
As an aside: Bayesians need a likelihood function for inference, and
frequentists need a likelihood function for analyzing their estimators.
The likelihood function is therefore central to all statistical methods,
and the core component of any data analysis.

In particular, the \CRLB\ is related to the Fisher Information that is
frequently used in cosmological experiment forecasts (\todo{CITE
  SHIT}).
It is that the \emph{inverse variance} $\sigma_v^{-2}$ of any velocity
estimator (that is, the information in the velocity estimator) must be smaller than the
second derivative of the \emph{negative} log likelihood function:
\begin{equation}
\frac{1}{\sigma_v^2} \leq E\left[\frac{\dd^2}{\dd v^2}(-\ln L)\right] \quad,
\end{equation}
where $L$ is the likelihood, or the probability of the data given the velocity,
and the expectation $E[\cdot]$ is taken over all possible data (that is, it is an
expectation under the likelihood).
If there are many other nuisance parameters, a Bayesian can marginalize them
out to make $L$ the marginalized likelihood for the velocity, or a frequentist
can include them in the estimation and compute an inverse covariance matrix $C^{-1}$
for the full parameter vector, which will have the bound
\begin{equation}
v^T\cdot C^{-1}\cdot v \leq v^T\cdot Q\cdot v
\end{equation}
\begin{equation}
Q_{ij} \equiv E\left[\frac{\dd^2}{\dd\theta_i\,\dd\theta_j}(-\ln L)\right] \quad,
\end{equation}
where $Q$ is the information tensor, $i,j$ are indices into that
tensor, corresponding parameters are $\theta_i, \theta_j$, and $v$ is
any arbitrary vector.  That is, the bound is not just on every
parameter, but really on on any combination of parameters.

In the case of \EPRV, this says that any computation of the limits
on estimating stellar velocities (or stellar velocity changes)
requires a likelihood function.
This, in turn, requires making assumptions about the stellar spectrum (the
latent, unobserved, \emph{true} spectrum that could only be observed
with arbitrarily good data) and the noise processes that distort the
spectrum in any finite, noisy observation.
These noise processes ought (in principle) to include any beliefs about
unsubtracted telluric absorptions or emissions, and time variability
of the star itself, not to mention photon and spectrograph read and
extraction noise.
However---and in keeping with the literature---we can make maximal or
best-case assumptions, and ask what we would get for \EPRV\ precision
in that best case.
Then we can ask (with fake data) how much different choices about our
data analysis, and different adversarial injections of noise,
tellurics, spectrograph issues, and other wrongness can prevent us
from achieving the best-case bounds.

The most optimistic assumptions we can possibly make are the following.
\begin{itemize}
\item The spectrograph is perfectly calibrated in a wavelength sense.
\item The spectrograph is consistently and repeatably calibrated in a
  flux sense. That is, the spectrum does not need to be perfectly
  continuum-normalized or perfectly flux-normalized, but it is
  calibrated near-perfectly \emph{consistently} across epochs (exposures).
\item There are no residual telluric absorptions nor any residual sky
  emission in the spectra. That is, the sky is near-perfectly calibrated
  and removed.
\item There is no time dependence (no epoch-to-epoch variations) in the
  stellar spectrum.
\item The \emph{true} spectrum (that is, the latent spectrum that
  would be observed if the data were far, far better) of the star is
  known at better accuracy and precision than the main noise sources.
\item All noise sources contributing to the spectral data---which
  include at a minimum photon noise, read noise, residual telluric
  issues, residual wavelength-calibration issues, and residual stellar
  spectral variability---are, when summed together, indistinguishable
  from being zero-mean, Gaussian, and additive.
\item The complete variance tensor of the total noise on the spectrum is known, and
  known correctly. That is, the variance of the noise at every pixel
  is known, as is the covariance between nearby pixels, and in the
  same \emph{true} sense that the true spectrum of the star is known.
\end{itemize}
It is worthy of note that every formula (that we know) in the
literature for maximum \RV\ precision (\todo{cite Butler, Bouchy,
  Figueira, Lovis}) makes these same assumptions, plus additional \emph{even
more restrictive assumptions}.
Under these disturbingly unrealistic assumptions, the \CRLB\ becomes
\begin{equation}\label{eq:crlb}
\frac{1}{\sigma_v^2} \leq \left[\frac{\dd f}{\dd v}\right]\T\cdot C^{-1}\cdot\left[\frac{\dd f}{\dd v}\right]
\end{equation}
where the derivatives are of the true spectrum with respect to
velocity (which can be thought of as the derivative of a Doppler
operator with respect to velocity acting on the true spectrum), the
derivatives are column vectors, and $C^{-1}$ is the inverse of the covariance matrix
describing the true variance and covariances of the noise process.

Formulae (bounds) related to (\ref{eq:crlb}) have appeared previously
in the literature, although in somewhat more complicated forms.
Butler (\todo{cite}) shows a similar expression, but makes the additional
assumption that every pixel is independent; that is, that there is no
covariance between nearby pixels from, say, extraction or continuum normalization.
Bouchy (\todo{cite}) shows an expression that assumes that the pixels
are independent and that the noise is dominated by a combination of photon
and read noise. 
That work also replaces the model spectrum with a set of optimal
weights for cross-correlation, which amounts to very much the same thing.
Figueira (\todo{cite}) and \todo{others (cite)} show a per-line
bound, based on assumptions similar to those of Bouchy but with the additional assumption that
each line is well approximated by a Gaussian (squared exponential) in shape.
These literature bounds are all extremely similar; nothing here is controversial.

Now that we have established the bound, how can we saturate it? That is,
how can we make measurements (estimators)  with variances that come close to the \CRLB?
The short answer is that reliable bound-saturating estimators are maximum-likelihood estimators.
The long answer is long, but part of it is that the bound can only be saturated if the
assumptions are correct, and note that the assumptions include that we know the
\emph{true} spectrum of the star, among many other things.
That is, if we can take data that come close to meeting the assumptions, and
if we can find a spectral model that comes close to the true spectrum of the star,
then in principle we can estimate radial velocities with a precision close to the \CRLB.

Given that computational stellar spectral models have physical issues, the best way
to get an accurate estimate of the true spectrum of the star is going to be data-driven.
In what follows, we are going to show that---for a typical \HARPS-like observing program---%
it is possible to build a data-driven model of the stellar spectrum that is good enough
to nearly-saturate the \CRLB.
That is, with a data-driven model of the true stellar spectrum, model inaccuracy will
not dominate the \RV\ noise budget.
We will additionally bring evidence that telluric features can be a significant problem,
but that they also can plausibly be mitigated with a data-driven model.

One final comment:
We are forced to perform these experiments primarily with artificial data.
The reason is: In order to assess the performance with respect to latent, unobservable \emph{truth},
we require knowledge of that latent, unobservable truth.
When we make artificial data, we play the r\^ole of God.
We will also show tests on two real stars, one hosting a companion and
one (apparently) companion-free (at least at our precision), but these
tests will necessarily only be confirmatory of the more stringent results
from artificial data.

\section{Overview of Methods}
\label{s:methods}

cross-correlation and max likelihood are the same: here's why. We are going to consider cross-correlation for simplicity in this paper.

for either of these, you could use:
\begin{itemize}
\item ``binary'' mask: reasons you might want to use this (automatically ignores tellurics, trivially computes average line profile), pipelines that use this \& how (quote HARPS papers)
\item synthetic spectrum (i.e. a slightly wrong model prediction, since the stellar model will never be a perfect reproduction of the star in question): sub-tweet: some surveys cross-correlate with a single (same) template for every single star, which is extremely wrong. you can be more right by using a spectral-type-specific template.
\item data-driven template spectrum: some surveys (HIRES) use a single observation as a template. combining information from many visits is better because the combined SNR is amazing.
\end{itemize}


discuss quadratic max procedure to get a fine-grained maximum from a coarse-grained cross-correlation grid


\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{binarymask}
\caption{Illustration of a weighted binary mask (blue windows) plotted over a small segment of the solar spectrum (black). The vertical length of each mask window corresponds to the weight given to the line when performing a cross-correlation between mask and spectrum.}
\label{fig:binarymask}
\end{figure}

\section{Data}
\label{s:data}

Most of what we do in this paper uses artificial data because we need access to the truth in order to compute the theoretical information content of the data. As a final test, we do use real data as a demonstration that the methods presented here are useful when applied to real data and the assumptions made in our artificial data generation are realistic.

\subsection{Artificial Data}

The basic model that we use to generate fake spectra is extremely simple, assuming a perfectly normalized continuum; isolated, perfectly Gaussian spectral lines; and white noise only. We simulated a 5-$\AA$ region with random noise corresponding to an SNR of 100, a typical value for a single \RV\ spectrum. Lines were inserted as Gaussians with arbitrarily-prescribed centers and depths (or equivalent widths). Every line was convolved with the same line spread function, which was taken to be a Gaussian with $\sigma$ = 0.05 $\AA$. %The number of lines used varies in the results discussed below.

We repeated this synthesis to make a set of \todo{64} spectra, each with a random \RV\ between $-30$ and $30\,\kms$ (the approximate amplitude of the yearly \RV\ shifts induced by barycentric motion). The \RV s can be applied as a straightforward Doppler shift in the centers of each line. \todo{(add equation?)} At this point, we are able to run a cross-correlation or a maximum-likelihood analysis on the set of spectra and examine the deviations in the recovered \RV s from the input \RV s.

discuss single-line and multi-line cases

FIGURE showing multi-line artificial data

\subsection{HARPS Data}

munging notes

FIGURE showing analogous section of real data

\section{Experiments}
\label{s:experiments}

\subsection{Single-Line}

We began with the simplest test case possible: a single, strong absorption feature.

What is the CRLB?

Discuss scaling of the CRLB with line FWHM, constrast/depth, etc. According to Lovis \& Fischer (2010) and Figueira (2017), $\sigma_{RV}$ should go as $\frac{\sqrt{FWHM}}{SNR \times C}$, where $C$ is the line contrast, or the depth of the line as a fraction of the continuum level \todo{(check on this definition of contrast)}. Do we see this?

\subsection{Binary Mask}

Our next test involved a more realistic test spectrum. We measured the centers and depths of \todo{N} lines in the Solar spectrum between {x - y} $\AA$ and used these measurements to generate a synthetic spectrum that closely resembles a typical observation of a Sun-like star.

What is the CRLB for this case? Binary mask cross-correlation is very far from saturating it. Cross-correlation or max-likelihood fitting with a perfect spectral template works much better. Talk a little bit about why.

\subsection{Rigid Template}

how well does the mask/template approximate the real data? try fudging the relative line depths and locations in both the binary mask and the template. do a correct template and an incorrect template.

even a tiny wrongness to the template is visible in the RV variance.

\subsection{Data-Driven Template}

how do we construct the template? show that it comes close to saturating the CRLB.

comment: there are far better ways to derive the data-driven template. we leave this to future work.

\subsection{Realistic Sources of Structured Noise}

additional noise considerations:
\begin{itemize}
\item (micro-) telluric features: tellurics that you can't identify are important
\item subtle continuum trends: not that big a deal
\item cosmic rays?: we might skip this and add to discussion
\item additive signals?: for example sky emission, lunar and other reflected solar; we might skip this and just add to discussion
\end{itemize}

\subsection{HARPS Data}

all the data-driven code works on real data. we get a better RMS on the star from the data-driven template than from the binary mask. based on this, we believe that the HARPS pipeline doesn't strictly use a binary mask.

\section{Summary \& Discussion}
\label{s:summary}

paragraph for each major result from the abstract

wrongness of our assumptions - how does this affect our conclusions?

limitations of our ``additional noise'' ideas and what we have considered.

Hogg: insert some grand final statement.

\acknowledgements
MPIA hospitality, Montet, Foreman-Mackey, Bean, Stuermer, Rix, HARPS team, ESO, Stars group meeting, Brewer

\bibliographystyle{apj}
\bibliography{}%general,myref,inprep}

\end{document}
