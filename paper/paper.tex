% This file is part of the EPRV project.
% Copyright 2017, 2018 the authors.

% Style notes
% - Re-define acronyms in each section? So less flippy flippy?

\documentclass[modern]{aastex62}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage[sort&compress]{natbib}
\usepackage[hang,flushmargin]{footmisc}

% units macros
\newcommand{\unit}[1]{\mathrm{#1}}
\newcommand{\km}{\unit{km}}
\newcommand{\m}{\unit{m}}
\newcommand{\s}{\unit{s}}
\newcommand{\kms}{\km\,\s^{-1}}
\newcommand{\ms}{\m\,\s^{-1}}
\newcommand{\ang}{\text{\normalfont\AA}}

% math macros
\newcommand{\dd}{\mathrm{d}}
\newcommand{\T}{^{\mathsf{T}}}

% text macros
\newcommand{\documentname}{\textsl{Article}}
\newcommand{\sectionname}{Section}
\newcommand{\todo}[1]{\textcolor{red}{#1}}  % gotta have \usepackage{xcolor} in main doc or this won't work
\newcommand{\acronym}[1]{{\small{#1}}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\HARPS}{\project{\acronym{HARPS}}}
\newcommand{\HIRES}{\project{\acronym{HIRES}}}
\newcommand{\RV}{\acronym{RV}}
\newcommand{\CRLB}{\acronym{CRLB}}

\setlength{\parindent}{1.4em} % trust in Hogg
\shorttitle{data analysis for radial-velocity measurement}
\shortauthors{bedell and hogg}

\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing % trust in Hogg
\graphicspath{ {figures/} }
\DeclareGraphicsExtensions{.pdf,.eps,.png}

\title{\textbf{Data analysis for maximum-precision radial-velocity mesurement}}

\author[0000-0001-9907-7742]{Megan Bedell}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Department of Astronomy \& Astrophysics, University of Chicago, 5640 S. Ellis Ave, Chicago, IL 60637, USA}

\author[0000-0003-2866-9403]{David W. Hogg}
\affil{Center for Computational Astrophysics, Flatiron Institute, 162 Fifth Ave, New York, NY 10010, USA}
\affil{Center for Cosmology and Particle Physics, Department of Physics, New York University, 726 Broadway, New York, NY 10003, USA}
\affil{Center for Data Science, New York University, 60 Fifth Ave, New York, NY 10011, USA}
\affil{Max-Planck-Institut f\"ur Astronomie, K\"onigstuhl 17, D-69117 Heidelberg}

\begin{abstract}\noindent
% context
Extremely precise stellar radial velocity (\RV) measurements are a
foundational tool in the field of exoplanets, with hundreds of planets
discovered by the Doppler method to date and many high-resolution
spectrographs around the world dedicated to search and
characterization.
Current observations are delivering \RV\ measurements at $\ms$
precision, while it is thought that some hardware systems ought to be
delivering data capable of even higher precisions.
% aims
The question arises: Is there any precision loss in current
data-analysis methods?
That is, what data-analysis methods deliver \RV\ measurements that
saturate the information-theoretic 
Cram\'er--Rao bound (\CRLB) on precision, given an extracted spectrum (and
associated uncertainties) from a spectrograph?
% methods
We consider cross-correlation with a binary mask, a smoothed (or
Gaussian-fit) cross-correlation with a binary mask, and
cross-correlation with a stellar template.
We consider both theoretically generated and data-driven templates,
and look at some methods for data-driven telluric removal.
% results
As expected, we find that cross-correlation with a correct stellar
template produces \RV\ measurements that saturate the \CRLB.
We find that pure cross-correlation with a binary mask under-performs,
but the standard practice of post-processing the binary-mask result
with a Gaussian fit does saturate the \CRLB, when the binary mask is
``correct'' in a certain sense.
As the template or mask becomes incorrect, \RV\ precision suffers.
Most importantly, we find that using the data themselves to generate
the stellar template does not cost any significant information; that
is, a data-driven template will be correct enough to saturate the
\CRLB, provided that there are multiple epochs of observations and the
star does not vary substantially between epochs.
As a side note, we find that tiny telluric features can affect \RV\ precision
substantially, but that they can be modeled and removed by a data-driven
method, provided that there is sufficient barycentric velocity variation to
separate the telluric and star rest frames.
\end{abstract}

\keywords{
atmospheric effects
---
binaries: spectroscopic
---
methods: data analysis
---
methods: statistical
---
techniques: radial velocities
---
planets and satellites: detection
}

\section{Introduction}

\todo{needs reduction in scope, perhaps?}

Accurate spectroscopic measurements of radial velocities (\RV s) have long been a critical tool across many subfields of astronomy, from Galactic dynamics to large-scale structure and cosmic expansion.
Fundamentally, accuracy of radial-velocity mesurement is limited by the quality of spectral models of stars, or templates.
In recent decades, however, a new focus has emerged on \emph{extremely precise} but purely \emph{relative} \RV\ measurements of stars as a means of detecting exoplanets and other kinds of companions.
For this application, measuring the absolute velocity of the star is much less important than resolving changes through time.
With sub-$\ms$-level precision required for the detection of Earth-like planets, vast amounts of effort have been poured into the engineering challenges of constructing a sufficiently stable spectrograph (or, failing that, being able to track the drift of a spectrograph over time) \todo{(cite some review papers)}. At the same time, great strides have been made towards disentangling true Doppler shifts from imitative signals like the spectral signatures of stellar pulsations and magnetic activity features \todo{(cite some review papers)}.  In this work, we investigate another fundamentally important but less commonly discussed aspect of the quest for maximum-precision \RV s: the method of analysis used to estimate the \RV\ given an extracted stellar spectrum.

While the general idea of measuring a Doppler shift by the relative motions of stellar absorption lines is fairly straightforward, there are considerable subtleties to implementing this, from the choice of reference spectrum or template to the treatment of telluric absorption features. Ultra-stabilized spectrographs like \HARPS\ \todo{(define \& cite)} predominantly utilize a cross-correlation method with a custom-built list of spectral lines (see e.g. Queloz1995, Baranne1996). 
A lesser-used alternative is cross-correlation or maximum-likelihood fitting using a template spectrum generated from the data (Buchhave, Anglada-Escude). 
This often involves masking or downweighting of the most telluric-contaminated spectral regions.

On the other hand, gas-cell-calibrated spectrographs like \HIRES\ commonly adopt a more comprehensive forward-modeling approach. 
In this framework, the instrumental line spread function, the stellar RV, and other parameters are optimized by modeling each observation as a combination of shifted and instrument-convolved template spectra for the star and gas cell. [say a bit more]

Our primary aim in this work is to establish a common frame of reference for these methods based on their relative ability to extract maximum-precision RVs and their robustness in the face of various expected structured noise sources.

For the purposes of this work, we assume that our data set consists of multiple observations of the same star with good coverage throughout the observing season, so that we have seen the star many times spanning the range of telluric-to-star relative RV shifts. 
This means we are not considering the case of a real-time data reduction pipeline. 
We also assume a \HARPS-like setup, i.e. no absorption cell, but the points we make can be easily generalized to the gas-cell case. 
We return to this point in the discussion \todo{(do this)}.

We begin in Section \ref{s:info} with a derivation of the information-theory-based limit on the RV information content of a spectrum. 
In Section \ref{s:methods}, we summarize common RV extraction practices used by current \RV\ pipelines, including cross-correlation and template fitting. 
We then implement these techniques to extract \RV s using 5-\ang-wide sections of simulated and real spectra, as described in Section \ref{s:data}. 
We compare the relative performances of the methods in Section \ref{s:experiments} and conclude with some recommendations for future RV pipelines in Section \ref{s:summary}. 

\section{Information Theory}
\label{s:info}

Given finite, noisy data, there is a limit coming from information
theory on how well any parameter of interest can be measured.
This limit is
sometimes known as the Cram\'er--Rao Lower Bound (\CRLB; \todo{cite CR and wikipedia}).
In its simplest form, it constrains the variance of any unbiased
frequentist estimator.
Estimators can only beat this bound by taking on bias.

This bound is strong, but it cannot be calculated \emph{objectively};
any description of the information in a data set requires
subjective decisions (that is, debatable decisions), which are inputs
to a model for the generation of the data.
In particular, information-theory bounds require a model for the
probability of the data given the parameter of interest; that is, they
require a likelihood function.
As an aside: Bayesians need a likelihood function for inference, and
frequentists need a likelihood function for analyzing their estimators.
The likelihood function is therefore central to all statistical methods,
and the core component of any data analysis.

In particular, the \CRLB\ is related to the Fisher Information that is
frequently used in cosmological experiment forecasts (\todo{CITE
  SHIT}).
It is that the \emph{inverse variance} $\sigma_v^{-2}$ of any velocity
estimator (that is, the information in the velocity estimator) must be smaller than the
second derivative of the \emph{negative} log likelihood function:
\begin{equation}
\frac{1}{\sigma_v^2} \leq E\left[\frac{\dd^2}{\dd v^2}(-\ln L)\right] \quad,
\end{equation}
where $L$ is the likelihood, or the probability of the data given the velocity,
and the expectation $E[\cdot]$ is taken over all possible data (that is, it is an
expectation under the likelihood).
If there are many other nuisance parameters, a Bayesian can marginalize them
out to make $L$ the marginalized likelihood for the velocity, or a frequentist
can include them in the estimation and compute an inverse covariance matrix $C^{-1}$
for the full parameter vector, which will have the bound
\begin{equation}
v^T\cdot C^{-1}\cdot v \leq v^T\cdot Q\cdot v
\end{equation}
\begin{equation}
Q_{ij} \equiv E\left[\frac{\dd^2}{\dd\theta_i\,\dd\theta_j}(-\ln L)\right] \quad,
\end{equation}
where $Q$ is the information tensor, $i,j$ are indices into that
tensor, corresponding parameters are $\theta_i, \theta_j$, and $v$ is
any arbitrary vector.  That is, the bound is not just on every
parameter, but really on on any combination of parameters.

In the case of extreme-precision \RV, this says that any computation of the limits
on estimating stellar velocities (or stellar velocity changes)
requires a likelihood function.
This, in turn, requires making assumptions about the stellar spectrum (the
latent, unobserved, \emph{true} spectrum that could only be observed
with arbitrarily good data) and the noise processes that distort the
spectrum in any finite, noisy observation.
These noise processes ought (in principle) to include any beliefs about
unsubtracted telluric absorptions or emissions, and time variability
of the star itself, not to mention photon and spectrograph read and
extraction noise.
However---and in keeping with the literature---we can make maximal or
best-case assumptions, and ask what we would get for \RV\ precision
in that best case.
Then we can ask (with fake data) how much different choices about our
data analysis, and different adversarial injections of noise,
tellurics, spectrograph issues, and other wrongness can prevent us
from achieving the best-case bounds.

The most optimistic assumptions we can possibly make are the following.
\begin{itemize}
\item The spectrograph is perfectly calibrated in a wavelength sense.
\item The spectrograph is consistently and repeatably calibrated in a
  flux sense. That is, the spectrum does not need to be perfectly
  continuum-normalized or perfectly flux-normalized, but it is
  calibrated near-perfectly \emph{consistently} across epochs (exposures).
  We will break this assumption in our experiments, below.
\item There are no residual telluric absorptions nor any residual sky
  emission in the spectra. That is, the sky is near-perfectly calibrated
  and removed.
  We will also break this assumption in our experiments, below.
\item The \emph{true} spectrum (that is, the latent spectrum that
  would be observed if the data were far, far better) of the star is
  known at better accuracy and precision than the main noise sources.
  We will break this assumption too.
  The whole point of this paper is to ask whether we can create a
  good-enough proxy for the true spectrum.
\item There is no time dependence (no epoch-to-epoch variations) in the
  stellar spectrum.
\item All noise sources contributing to the spectral data---which
  include at a minimum photon noise, read noise, residual telluric
  issues, residual wavelength-calibration issues, and residual stellar
  spectral variability---are, when summed together, indistinguishable
  from being zero-mean, Gaussian, and additive.
\item The complete variance tensor of the total noise on the spectrum is known, and
  known correctly. That is, the variance of the noise at every pixel
  is known, as is the covariance between nearby pixels, and in the
  same \emph{true} sense that the true spectrum of the star is known.
\end{itemize}
It is worthy of note that every formula (that we know) in the
literature for maximum \RV\ precision (\todo{cite Butler, Bouchy,
  Figueira, Lovis}) makes all of these same assumptions,
plus additional \emph{even more restrictive assumptions}.
Under these disturbingly unrealistic assumptions, the \CRLB\ becomes
\begin{equation}\label{eq:crlb}
\frac{1}{\sigma_v^2} \leq \left[\frac{\dd f}{\dd v}\right]\T\cdot C^{-1}\cdot\left[\frac{\dd f}{\dd v}\right]
\end{equation}
where the derivatives are of the true spectrum with respect to
velocity (which can be thought of as the derivative of a Doppler
operator with respect to velocity acting on the true spectrum), the
derivatives are column vectors, and $C^{-1}$ is the inverse of the covariance matrix
describing the true variance and covariances of the noise process.

Formulae (bounds) related to (\ref{eq:crlb}) have appeared previously
in the literature, although in somewhat more complicated forms.
Butler (\todo{cite}) shows a similar expression, but makes the additional
assumption that every pixel is independent; that is, that there is no
covariance between nearby pixels from, say, extraction or continuum normalization.
Bouchy (\todo{cite}) shows an expression that assumes that the pixels
are independent and that the noise is dominated by a combination of photon
and read noise. 
That work also replaces the model spectrum with a set of optimal
weights for cross-correlation, which amounts to very much the same thing.
Figueira (\todo{cite}) and \todo{others (cite)} show a per-line
bound, based on assumptions similar to those of Bouchy but with the additional assumption that
each line is well approximated by a Gaussian (squared exponential) in shape.
These literature bounds are all extremely similar, and similar to (\ref{eq:crlb});
nothing here is controversial.
We will come back to the scaling of uncertainty with various Gaussian line parameters in \sectionname~\ref{s:singleline}.

Now that we have established the bound, how can we saturate it? That is,
how can we make measurements (estimators)  with variances that come close to the \CRLB?
The short answer is that reliable bound-saturating estimators are maximum-likelihood estimators.
The long answer is long, but part of it is that the bound can only be saturated if the
assumptions are correct, and note that the assumptions include that we know the
\emph{true} spectrum of the star, among many other things.
That is, if we can take data that come close to meeting the assumptions, and
if we can find a spectral model that comes close to the true spectrum of the star,
then in principle we can estimate radial velocities with a precision close to the \CRLB.

Given that computational stellar spectral models have physical issues, the best way
to get an accurate estimate of the true spectrum of the star is going to be data-driven.
In what follows, we are going to show that---for a typical \HARPS-like observing program---%
it is possible to build a data-driven model of the stellar spectrum that is good enough
to nearly-saturate the \CRLB.
That is, with a data-driven model of the true stellar spectrum, model inaccuracy will
not dominate the \RV\ noise budget.
We will additionally bring evidence that telluric features can be a significant problem,
but that they also can plausibly be mitigated with a data-driven model.

\section{Radial-velocity estimators}
\label{s:methods}

Most methods for radial-velocity estimation involve some kind of
cross-correlation of the data with a template.
This might be surprising, since (as we noted above) bound-saturating
estimators will be maximum-likelihood estimators.
However, under some simple assumptions, cross-correlation optima are
also maximum-likelihood optima.

For example, when the likelihood is Gaussian, the log-likelihood
contains a chi-squared-like term
\begin{equation}
-\frac{1}{2}\,[y - f]\T\cdot C^{-1}\cdot [y - f] \quad ,
\end{equation}
where $y$ is a column vector of the data, $f$ is the model spectrum,
and $C$ is the covariance matrix describing the noise variance.
In the case of independent pixel noise this just becomes an
inverse-variance weighted sum of squared residuals (chi-squared).
If we expand this quadratic form, it becomes
\begin{equation}
{}-\frac{1}{2}\,y\T\cdot C^{-1}\cdot y + y^T\cdot C^{-1}\cdot f 
-\frac{1}{2}\,f\T\cdot C^{-1}\cdot f \quad.
\end{equation}
That is, the chi-squared term factors into a term that is the square
of the model, a term that is the square of the data, and a term that
is a cross-correlation.
As long as the amplitude of $f$ doesn't depend on velocity, and the
$C$ tensor is ``smooth'' in certain respects, the dependence of this
chi-squared term on velocity will be dominated by the
cross-correlation.
For this reason, we will use a variant of cross-correlation
optimization as our radial-velocity estimator.

...In detail, the cross-correlation we will optimize is...

A variety of choices exist for the cross-correlation template $f$. A few possibilities include:
\begin{itemize}
\item a ``binary'' mask, or a spectrum of zero value with non-zero peaks at the expected rest-frame wavelengths of the spectral lines of interest. A cross-correlation between this mask and the observed spectrum will effectively compute the average line profile with its minimum corresponding to the best-fit RV. This method has the benefit of trivially excluding telluric-contaminated or otherwise unreliable regions of the spectrum by deleting them from the mask. A desired relative weighting of lines can be incorporated by setting the non-zero mask values to different amplitudes at different lines (cite Pepe). This method is currently used by the \HARPS\ pipeline (cite).
\item synthetic spectrum (i.e. a slightly wrong model prediction, since the stellar model will never be a perfect reproduction of the star in question): sub-tweet: some surveys cross-correlate with a single (same) template for every single star, which is extremely wrong. you can be more right by using a spectral-type-specific template.
\item data-driven template spectrum: some surveys (HIRES) use a single observation as a template. combining information from many visits is better because the combined SNR is amazing.
\end{itemize}

One final implementation detail is the following:
When we optimize the cross-correlation function, we don't run an optimizer;
we perform brute-force search on a linear velocity grid.
This grid, however, does not need to be fine.
We use a coarse grid (\todo{give grid spacing here}), which does not
over-resolve the velocity peak at the optimum.
We then take the highest value on the grid and the values on either
side, fit a parabola through those three values, and take as the optimal
velocity the peak of that parabola.
That is, we obtain sub-grid velocity resolution by means of a quadratic
interpolation at the peak on the finite grid.
\todo{Do we need to demonstrate this with a toy figure?}

\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{binarymask}
\caption{Illustration of a weighted binary mask (blue windows) plotted over a small segment of the solar spectrum (black). The vertical length of each mask window corresponds to the weight given to the line when performing a cross-correlation between mask and spectrum.}
\label{fig:binarymask}
\end{figure}

\section{Data}
\label{s:data}

The experiments we perform in this paper primarily use artificial data, for the simple reason that we require access to the underlying true RVs in order to assess the performance of our methods. 
We attempt to generate the fake data as realistically as possible for the case of a high-resolution, high-signal-to-noise spectrum of a Sun-like star taken with \HARPS. 
With these data, we are able to compute the \CRLB.  
We can also isolate the effects of various confounding noise sources like telluric features and imperfect continuum normalization by injecting them into the artificial data, which is generated free of correlated noise by default.

We will also show tests on real \HARPS\ data for two stars, one hosting a companion and
one (apparently) companion-free (at least at our precision). 
These tests serve as a demonstration that the methods presented here are useful when applied to real data and the assumptions made in our artificial data generation are realistic.

\subsection{Artificial Data}

The basic model that we use to generate fake spectra is extremely simple, assuming a perfectly normalized continuum; isolated, perfectly Gaussian spectral lines; and white noise only. 
We simulated a 5-\ang\ region with random noise corresponding to an SNR of 100, a typical value for a single \RV\ spectrum. 
Lines were inserted as Gaussians with prescribed centers, widths, and depths. 
Every line was convolved with the same line spread function, which was taken to be a Gaussian with $\sigma$ = 0.05 \ang. 
In the simplest test case, an arbitrary single line at 5000 \ang\ was used (see Section \ref{s:singleline}). 
However, for most of our tests we injected a set of 12 lines with centers between 4998 - 5002 \ang. 
The Gaussian parameters for these lines were based on fits to a reflected Solar spectrum from Vesta taken by \HARPS. \todo{(find citation/program number)} 
The resulting spectrum strongly resembles a 5-\ang\ chunk taken from a typical \HARPS\ spectrum of a Sun-like star (Figure \ref{fig:spectra}).

We repeated this synthesis to make a set of \todo{64} spectra, each with a random \RV\ between $-30$ and $30\,\kms$ (the approximate amplitude of the yearly \RV\ shifts induced by barycentric motion). 
The \RV s were applied as a multiplicative Doppler shift on the central wavelength of each spectral line to be injected. 
At this point, we are able to run a cross-correlation analysis on the set of spectra and examine the deviations in the recovered \RV s from the input \RV s.

\begin{figure}
\centering
% \includegraphics[width=\columnwidth]{spectra}
\caption{}
\label{fig:spectra}
\end{figure}

\subsection{\HARPS\ Data}

munging notes

reference to figure comparing real \& fake data

\section{Experiments and results}
\label{s:experiments}

\subsection{Single-Line}
\label{s:singleline}

We began with the simplest test case possible: a single, strong absorption feature.

What is the CRLB?

Discuss scaling of the CRLB with line FWHM, constrast/depth, etc. According to Lovis \& Fischer (2010) and Figueira (2017), $\sigma_{RV}$ should go as $\frac{\sqrt{FWHM}}{SNR \times C}$, where $C$ is the line contrast, or the depth of the line as a fraction of the continuum level. Do we see this?

\subsection{Binary Mask}

Our next test involved a more realistic test spectrum. Using \HARPS\ spectra of the solar twin star HIP 54287, we measured the centers and depths of 12 lines in a 4-$\ang$ window around 5000 $\ang$. We used these measurements to generate a synthetic spectrum that closely resembles a typical observation of a Sun-like star.

What is the CRLB for this case? How close can we come to saturating it when using (a) a typical HARPS-like mask and (b) an optimized mask? In all cases, binary mask cross-correlation is very far from saturating the CRLB. Talk a little bit about why.

\subsection{Rigid Template}

how well does the mask/template approximate the real data? try fudging the relative line depths and locations believably, like maybe you have a slightly wrong spectral type or metallicity in the template. do a correct template and an incorrect template.

even a tiny wrongness to the template is visible in the RV variance.

\subsection{Data-Driven Template}

how do we construct the template? show that it comes close to saturating the CRLB.

comment: there are far better ways to derive the data-driven template. we leave this to future work.

\subsection{Realistic Sources of Structured Noise}

additional noise considerations:
\begin{itemize}
\item (micro-) telluric features: tellurics that you can't identify are important
\item subtle continuum trends: not that big a deal
\item cosmic rays?: we might skip this and add to discussion
\item additive signals?: for example sky emission, lunar and other reflected solar; we might skip this and just add to discussion
\end{itemize}

\subsection{Real Data}

all the data-driven code works on real data. we get a better RMS on the star from the data-driven template than from the binary mask. based on this, we believe that the \HARPS\ pipeline doesn't strictly use a binary mask.

\section{Summary and discussion}
\label{s:summary}

This \documentname\ reports good news for exoplanet-related
radial-velocity (\RV) experiments.
First and foremost, we have shown that a data-driven model for
stellar spectra can be accurate enough to deliver
\RV\ measurements that saturate information-theoretic
bounds.
It is not surprising that an accurate model of a stellar spectrum
yields good cross-correlation \RV\ measurements.
What is interesting (at least to us) is that there is sufficient
spectral information in a typical observing
program to determine the mean stellar spectrum to sufficient
quality that the \RV\ measurements are close to the best that can
theoretically be made.
It means that the information in the data that goes into determining
the mean stellar spectrum does not come at the expense (in any sense)
of the \RV\ information.
It also means that \RV\ experiments do not rely on having good models
of stellar spectra; no physical model of a star or its photosphere
is required.

...insert here one paragraph for each of the other major results from
the abstract. Each paragraph should emphasize the awesome of the
result, and/or discuss limitations or interpretation...

In \sectionname~\ref{s:info} we listed some exceedingly
optimistic assumptions under which we could compute the
information-theoretic limit on radial-velocity precision.
Some of these assumptions are worthy of discussion: How true are they,
and what happens, both from an information-theory perspective and from
a methodological perspective, when we relax them?

One of these assumptions, for example, is that the true stellar
spectrum is constant with time; that is, the variations from
epoch-to-epoch are from noise or nuisances (like telluric absorption).
This assumption is critical and deep.
The entire edifice of extreme-precision \RV\ is built on this assumption:
With imperfect models of stellar structure and photospheres, it is
impossible to know the \emph{absolute} \RV\ of a star to high precision;
we satisfy ourselves that we can measure \emph{relative} velocities
to arbitrarily high precision.
Relative precision---and everything in this paper---depends on our
ability to measure shifts from epoch to epoch.
If the stellar spectrum is permitted (or observed) to change from epoch
to epoch, there is no fundamental way to determine or confirm spectral
shifts corresponding to Doppler shifts.
That is, even relative \RV\ measurements depend on a constancy of the
stellar spectrum.
This is a deep issue, extending far beyond the scope of this paper.
Time-variable stellar spectra are not killing to the extreme-precision
\RV\ program!
But at current practices they may be limiting precision,
and to ameliorate their effects will require substantial and non-trivial adjustments to the
\foreign{status quo}.
We hope to address these in future work.

We also assumed that the noise is zero-mean, Gaussian, and with known
variance tensor.
There are certainly sources of noise that might violate these
assumptions.
There are two ways in which we might weaken our assumptions about
the noise.
The first is that the total noise acting on the system---and recall
that noise for us is anything not accounted for in our model of the
data---might not be Gaussian in form.
This will happen if the photon counts (per pixel) get low.
But it can also happen if there are unmodeled calibration artifacts
that don't happen to have Gaussian-distributed amplitudes.
This is not a problem in principle with computing or saturating the
information-theory limit (\CRLB).
The only significant change is that the simple Gaussian likelihood must
be replaced with the appropriate function.
That change modifies the \CRLB\ calculation, and it also modifies the
relevant data-analysis method, since cross-correlation (justified in
\sectionname~\ref{s:methods}) is only appropriate when the noise is
close to Gaussian.
In a non-Gaussian world, the \RV\ estimation must proceed by explicit
optimization of the likelihood (or Bayesian methods derived therefrom).

The second way in which we might weaken our assumptions about the
noise is that there might be contributions to the noise (especially
those coming from unmodeled calibration artifacts) with unknown
variance.
That is, it is an optimistic assumption that we know the variance of
the noise, and all the covariances.
Our only comment about this problem is that, at least, the assumption
is testable:
It is possible with big observing programs to look at the empirical
properties of the residuals away from the best-fit model.
Do the residuals appear to have zero mean and reasonable empirical
variance, given the assumed variance?
These tests are like chi-squared tests; they test the completeness
of the model.
If the model is incomplete, it can be fixed by adding new components
to the model (calibration vectors of various kinds, for example) or
inflating estimates of noise variance. Or both.

...Hogg: insert some grand final statement.

\acknowledgements
It is a pleasure to thank
  Jacob Bean (Chicago),
  John Brewer (Yale),
  Dan Foreman-Mackey (Flatiron),
  Ben Montet (Chicago),
  Hans-Walter Rix (MPIA),
  Julian Stuermer (Chicago),
and the participants in the weekly Stars Group Meeting at the Flatiron
Institute for valuable contributions to this project.
None of this would have been possible without the incredible effort
of the \HARPS\ team and \acronym{ESO}, who built the instrument and made the
data used here public.
MB thanks the Max-Planck-Institut f\"ur Astronomie in Heidelberg
for hospitality during a critical phase of this project.

\todo{...add Hogg grant numbers}

\todo{...add facilities tags}

\todo{...add software tags}

\bibliographystyle{apj}
\bibliography{}%general,myref,inprep}

\end{document}
